{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fear emotion\n",
    "\n",
    "#데이터 pandas로 읽기, 구문이 탭으로 구분되어있음.\n",
    "import pandas as pd\n",
    "train_fear_df = pd.read_csv(\"feartrainalldata.txt\",\"\\t\") \n",
    "test_fear_df = pd.read_csv(\"feartestalldata.txt\",\"\\t\")\n",
    "\n",
    "#konlpy는 띄어쓰기 알고리즘과 정규화를 이용해 맞춤법이 틀린 문장을 고치고 형태소 분석과 품사를 태깅해주는 클래스를 제공함.\n",
    "from konlpy.tag import Okt\n",
    "okt_fear = Okt()\n",
    "# Okt를 쓴 이유 : 사용한 데이터는 인터넷 댓글이다. 어느정도 띄어쓰기가 되어있는 상태이기에 Okt가 가장 유리하다.\n",
    "\n",
    "def fear_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_fear.pos(doc, norm=True, stem=False)]#08.22 stem=false로 바꿔봄. \n",
    "    #'구분자'.join(list) => 리스트 값과 값 사이에 구분자를 넣어 문자열을 하나로 합침. ex) '_'.join(['a','b','c'])이면 \"a_b_c\"로 반환\n",
    "    #pos(text) => 품사를 태깅한 상태로 명사를 변환, ex) 컴퓨터(x), 컵퓨터,noun(o)\n",
    "    #norm => 정규화 ex) 그래욬ㅋㅋㅋ => 그래요\n",
    "    #stem => 근어로 표현 ex) 그래요 => 그렇다\n",
    "\n",
    "train_fear_df.isnull().any()\n",
    "train_fear_df['document'] = train_fear_df['document'].fillna(''); #null값을 ''로 대체\n",
    "test_fear_df.isnull().any()\n",
    "test_fear_df['document'] = test_fear_df['document'].fillna('');\n",
    "\n",
    "train_fear_docs = [(fear_tokenize(row[1]), row[2]) for row in train_fear_df.values] #traindata 저장 row[1] : document / row[2] : label\n",
    "test_fear_docs = [(fear_tokenize(row[1]),row[2]) for row in test_fear_df.values] #testdata 저장\n",
    "\n",
    "tokens = [t for d in train_fear_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "fear_text = nltk.Text(tokens, name='FEAR') \n",
    "#문서 하나를 편리하게 탐색할 수 있는 기능 제공 (vocab().most_common() 사용)\n",
    "# print(len(fear_text.tokens)) #토큰 개수\n",
    "# print(len(set(fear_text.tokens))) #중복을 제외한 토큰 수\n",
    "# print(fear_text.vocab().most_common(10)) #출력빈도가 높은 상위 토큰 10개\n",
    "\n",
    "#countvectorization\n",
    "FEAR_FREQUENCY_COUNT = 3000; #자주 사용되는 토큰 설정 3000이상 하면 메모리 에러가 발생한다.\n",
    "fear_selected_words = [f[0] for f in fear_text.vocab().most_common(FEAR_FREQUENCY_COUNT)] #선택되어진 토큰들\n",
    "\n",
    "def fear_term_frequency(doc):\n",
    "    return [doc.count(word) for word in fear_selected_words]\n",
    "\n",
    "x_fear_train = [fear_term_frequency(d) for d,_ in train_fear_docs]\n",
    "x_fear_test = [fear_term_frequency(d) for d,_ in test_fear_docs]\n",
    "#x축에는 문서에 들어가는 단어 개수(단어들의 빈도수 정보)\n",
    "\n",
    "y_fear_train = [c for _,c in train_fear_docs]\n",
    "y_fear_test = [c for _,c in test_fear_docs]\n",
    "#y축에는 1 or 0, 분류 결과\n",
    "\n",
    "import numpy as np\n",
    "x_fear_train = np.asarray(x_fear_train).astype('float32')\n",
    "x_fear_test = np.asarray(x_fear_test). astype('float32')\n",
    "\n",
    "y_fear_train = np.asarray(y_fear_train).astype('float32')\n",
    "y_fear_test = np.asarray(y_fear_test).astype('float32')\n",
    "#np.asarray는 np.array와 달리 데이터 형태가 같을 때 복사하지 않음.\n",
    "#데이터 float로 형 변환\n",
    "\n",
    "import tensorflow as tf #텐서플로 케라스\n",
    "fear_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(FEAR_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "#레이어 구성은 dense층은 64개의 유닛, 활성함수는 relu, 마지막 층은 sigmoid 활성화 함수 사용\n",
    "# relu를 쓴 이유 : 연산의 cost가 적고, 구현이 간단함.\n",
    "# sigmoid를 쓴 이유 : 다중 감정 분석이지만 이진 분류 문제로 해결했기 때문에 출력층의 활성화함수는 이진 분류 문제에 효과적인 시그모이드 사용 \n",
    "# relu층이 두 층인 이유 : 세 층으로 했을 때, testset에 대한 정확도가 떨어졌다. 단순히 하는 것이 overfitting을 막는 방법\n",
    "# dense층이 64개의 유닛인 이유는?\n",
    "\n",
    "fear_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "#손실 함수는 binary_crossentropy, RMSprop 옵티마이저를 통해 경사하강법 진행\n",
    "#binary_crossentropy를 사용한 이유는 가장 이진 분류에 가장 적합하기 때문이다.\n",
    "#RMSprop와 Adam은 거의 비슷하다. Adam을 쓰면 메모리 에러가 발생함.\n",
    "\n",
    "fear_model.fit(x_fear_train, y_fear_train, epochs=50, batch_size=256)\n",
    "#배치 사이즈 줄이면 한 번에 판단하는 데이터 수 증가함. 에포크 50번\n",
    "#에포크에 따른 손실함수 그래프를 통해 50번이 적당하다고 판단하였다.\n",
    "\n",
    "fear_results = fear_model.evaluate(x_fear_test, y_fear_test)\n",
    "\n",
    "fear_review = \"너무 무섭다\"\n",
    "fear_token = fear_tokenize(fear_review)\n",
    "\n",
    "fear_tf = fear_term_frequency(fear_token)\n",
    "f_data = np.expand_dims(np.asarray(fear_tf).astype('float32'),axis=0)\n",
    "float(fear_model.predict(f_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
