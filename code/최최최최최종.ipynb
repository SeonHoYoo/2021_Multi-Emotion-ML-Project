{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fifth-lightweight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259374\n",
      "20799\n",
      "[('하다/Verb', 8087), ('이/Josa', 4939), ('./Punctuation', 4679), ('가/Josa', 3509), ('?/Punctuation', 3201), ('에/Josa', 3168), ('들/Suffix', 3097), ('../Punctuation', 2820), ('을/Josa', 2523), ('은/Josa', 2512)]\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 6s 107ms/step - loss: 0.4318 - binary_accuracy: 0.8487\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.3167 - binary_accuracy: 0.8710\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2829 - binary_accuracy: 0.8859\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2648 - binary_accuracy: 0.8939\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2506 - binary_accuracy: 0.8988\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2368 - binary_accuracy: 0.9065\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2225 - binary_accuracy: 0.9131\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2077 - binary_accuracy: 0.9216\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1923 - binary_accuracy: 0.9263\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1760 - binary_accuracy: 0.9348\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.1596 - binary_accuracy: 0.9417\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.1426 - binary_accuracy: 0.9494\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.1257 - binary_accuracy: 0.9569\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.1078 - binary_accuracy: 0.9636\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0921 - binary_accuracy: 0.9706\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0771 - binary_accuracy: 0.9760\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0641 - binary_accuracy: 0.9807\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0530 - binary_accuracy: 0.9846\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0441 - binary_accuracy: 0.9869\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0368 - binary_accuracy: 0.9893\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0308 - binary_accuracy: 0.9910\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0265 - binary_accuracy: 0.9923\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0223 - binary_accuracy: 0.9935\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0191 - binary_accuracy: 0.9942\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0168 - binary_accuracy: 0.9948\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0146 - binary_accuracy: 0.9955\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0132 - binary_accuracy: 0.9959\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0118 - binary_accuracy: 0.9962\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0108 - binary_accuracy: 0.9964\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0101 - binary_accuracy: 0.9968\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0093 - binary_accuracy: 0.9968\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 1s 9ms/step - loss: 0.0090 - binary_accuracy: 0.9969\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0087 - binary_accuracy: 0.9970\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0083 - binary_accuracy: 0.9972\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0080 - binary_accuracy: 0.9974\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0076 - binary_accuracy: 0.9973\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0076 - binary_accuracy: 0.9971\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0074 - binary_accuracy: 0.9972\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0075 - binary_accuracy: 0.9971\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0072 - binary_accuracy: 0.9971\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0069 - binary_accuracy: 0.9974\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0066 - binary_accuracy: 0.9974\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0069 - binary_accuracy: 0.9974\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0069 - binary_accuracy: 0.9973\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0069 - binary_accuracy: 0.9973\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0063 - binary_accuracy: 0.9976\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0063 - binary_accuracy: 0.9976\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0068 - binary_accuracy: 0.9973\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0065 - binary_accuracy: 0.9974\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0063 - binary_accuracy: 0.9975\n",
      "362/362 [==============================] - 2s 5ms/step - loss: 1.7516 - binary_accuracy: 0.8301\n",
      "공포(20%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_fear_df = pd.read_csv(\"feartrainalldata.txt\",\"\\t\")\n",
    "test_fear_df = pd.read_csv(\"feartestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_fear = Okt()\n",
    "\n",
    "def fear_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_fear.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_fear_df.isnull().any()\n",
    "train_fear_df['document'] = train_fear_df['document'].fillna('');\n",
    "train_fear_df.isnull().any()\n",
    "test_fear_df['document'] = test_fear_df['document'].fillna('');\n",
    "\n",
    "train_fear_docs = [(fear_tokenize(row[1]), row[2]) for row in train_fear_df.values]\n",
    "test_fear_docs = [(fear_tokenize(row[1]),row[2]) for row in test_fear_df.values]\n",
    "\n",
    "tokens = [t for d in train_fear_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "fear_text = nltk.Text(tokens, name='NMSC')\n",
    "print(len(fear_text.tokens))\n",
    "print(len(set(fear_text.tokens)))\n",
    "print(fear_text.vocab().most_common(10))\n",
    "\n",
    "FEAR_FREQUENCY_COUNT = 2000;\n",
    "fear_selected_words = [f[0] for f in fear_text.vocab().most_common(FEAR_FREQUENCY_COUNT)]\n",
    "\n",
    "def fear_term_frequency(doc):\n",
    "    return [doc.count(word) for word in fear_selected_words]\n",
    "\n",
    "x_fear_train = [fear_term_frequency(d) for d,_ in train_fear_docs]\n",
    "x_fear_test = [fear_term_frequency(d) for d,_ in test_fear_docs]\n",
    "y_fear_train = [c for _,c in train_fear_docs]\n",
    "y_fear_test = [c for _,c in test_fear_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_fear_train = np.asarray(x_fear_train).astype('float32')\n",
    "x_fear_test = np.asarray(x_fear_test). astype('float32')\n",
    "\n",
    "y_fear_train = np.asarray(y_fear_train).astype('float32')\n",
    "y_fear_test = np.asarray(y_fear_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "fear_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(FEAR_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "fear_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "fear_model.fit(x_fear_train, y_fear_train, epochs=50, batch_size=512)\n",
    "fear_results = fear_model.evaluate(x_fear_test, y_fear_test)\n",
    "fear_review = \"너무 무섭다\"\n",
    "fear_token = fear_tokenize(fear_review)\n",
    "\n",
    "fear_tf = fear_term_frequency(fear_token)\n",
    "f_data = np.expand_dims(np.asarray(fear_tf).astype('float32'),axis=0)\n",
    "float(fear_model.predict(f_data))\n",
    "\n",
    "def fear_predict(predict):\n",
    "    fear_token = fear_tokenize(predict)\n",
    "    fear_tfq = fear_term_frequency(fear_token)\n",
    "    fear_data = np.expand_dims(np.asarray(fear_tfq).astype('float32'), axis=0)\n",
    "    fear_score = float(fear_model.predict(fear_data))\n",
    "    print(f\"공포({round(fear_score*100)}%)\")\n",
    "    fear = round(fear_score*100)\n",
    "\n",
    "fear_predict(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "velvet-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공포(0%)\n"
     ]
    }
   ],
   "source": [
    "fear_predict(\"눈물이 난다...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stone-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259374\n",
      "20799\n",
      "[('하다/Verb', 8087), ('이/Josa', 4939), ('./Punctuation', 4679), ('가/Josa', 3509), ('?/Punctuation', 3201), ('에/Josa', 3168), ('들/Suffix', 3097), ('../Punctuation', 2820), ('을/Josa', 2523), ('은/Josa', 2512)]\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 0.4495 - binary_accuracy: 0.8341\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.3488 - binary_accuracy: 0.8529\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3149 - binary_accuracy: 0.8666\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2947 - binary_accuracy: 0.8760\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2765 - binary_accuracy: 0.8846\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2586 - binary_accuracy: 0.8937\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2396 - binary_accuracy: 0.9021\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2200 - binary_accuracy: 0.9113\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1994 - binary_accuracy: 0.9218\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1797 - binary_accuracy: 0.9309\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1598 - binary_accuracy: 0.9396\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1405 - binary_accuracy: 0.9481\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1214 - binary_accuracy: 0.9561\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1029 - binary_accuracy: 0.9639\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0874 - binary_accuracy: 0.9702\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0729 - binary_accuracy: 0.9759\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0605 - binary_accuracy: 0.9797\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0504 - binary_accuracy: 0.9837\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0424 - binary_accuracy: 0.9869\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0363 - binary_accuracy: 0.9882\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0309 - binary_accuracy: 0.9896\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0262 - binary_accuracy: 0.9916\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0236 - binary_accuracy: 0.9916\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0209 - binary_accuracy: 0.9928\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0182 - binary_accuracy: 0.9938\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0160 - binary_accuracy: 0.9948\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0146 - binary_accuracy: 0.9946\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - binary_accuracy: 0.9951\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - binary_accuracy: 0.9956\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - binary_accuracy: 0.9959\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - binary_accuracy: 0.9961\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - binary_accuracy: 0.9960\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - binary_accuracy: 0.9963\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - binary_accuracy: 0.9963\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0093 - binary_accuracy: 0.9964\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - binary_accuracy: 0.9964\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0087 - binary_accuracy: 0.9968\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0086 - binary_accuracy: 0.9966\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0085 - binary_accuracy: 0.9969\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0081 - binary_accuracy: 0.9967\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0083 - binary_accuracy: 0.9967\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0077 - binary_accuracy: 0.9965\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0077 - binary_accuracy: 0.9969\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0081 - binary_accuracy: 0.9965\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0078 - binary_accuracy: 0.9967\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0078 - binary_accuracy: 0.9966\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0076 - binary_accuracy: 0.9967\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0075 - binary_accuracy: 0.9966\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0075 - binary_accuracy: 0.9966\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0077 - binary_accuracy: 0.9963\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.9182 - binary_accuracy: 0.8171\n",
      "놀람(98%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_surprise_df = pd.read_csv(\"surprisetrainalldata.txt\",\"\\t\")\n",
    "test_surprise_df = pd.read_csv(\"surprisetestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_surprise = Okt()\n",
    "\n",
    "def surprise_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_surprise.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_surprise_df.isnull().any()\n",
    "train_surprise_df['document'] = train_surprise_df['document'].fillna('');\n",
    "train_surprise_df.isnull().any()\n",
    "test_surprise_df['document'] = test_surprise_df['document'].fillna('');\n",
    "\n",
    "train_surprise_docs = [(surprise_tokenize(row[1]), row[2]) for row in train_surprise_df.values]\n",
    "test_surprise_docs = [(surprise_tokenize(row[1]),row[2]) for row in test_surprise_df.values]\n",
    "\n",
    "tokens = [t for d in train_surprise_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "surprise_text = nltk.Text(tokens, name='NMSC')\n",
    "print(len(surprise_text.tokens))\n",
    "print(len(set(surprise_text.tokens)))\n",
    "print(surprise_text.vocab().most_common(10))\n",
    "\n",
    "SURPRISE_FREQUENCY_COUNT = 2000;\n",
    "surprise_selected_words = [f[0] for f in surprise_text.vocab().most_common(SURPRISE_FREQUENCY_COUNT)]\n",
    "\n",
    "def surprise_term_frequency(doc):\n",
    "    return [doc.count(word) for word in surprise_selected_words]\n",
    "\n",
    "x_surprise_train = [surprise_term_frequency(d) for d,_ in train_surprise_docs]\n",
    "x_surprise_test = [surprise_term_frequency(d) for d,_ in test_surprise_docs]\n",
    "y_surprise_train = [c for _,c in train_surprise_docs]\n",
    "y_surprise_test = [c for _,c in test_surprise_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_surprise_train = np.asarray(x_surprise_train).astype('float32')\n",
    "x_surprise_test = np.asarray(x_surprise_test). astype('float32')\n",
    "\n",
    "y_surprise_train = np.asarray(y_surprise_train).astype('float32')\n",
    "y_surprise_test = np.asarray(y_surprise_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "surprise_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(SURPRISE_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "surprise_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "surprise_model.fit(x_surprise_train, y_surprise_train, epochs=50, batch_size=512)\n",
    "surprise_results = surprise_model.evaluate(x_surprise_test, y_surprise_test)\n",
    "surprise_review = \"너무 놀랍다\"\n",
    "surprise_token = surprise_tokenize(surprise_review)\n",
    "\n",
    "tf = surprise_term_frequency(surprise_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(surprise_model.predict(data))\n",
    "\n",
    "def surprise_predict(predict):\n",
    "    surprise_token = surprise_tokenize(predict)\n",
    "    surprise_tfq = surprise_term_frequency(surprise_token)\n",
    "    surprise_data = np.expand_dims(np.asarray(surprise_tfq).astype('float32'), axis=0)\n",
    "    surprise_score = float(surprise_model.predict(surprise_data))\n",
    "    print(f\"놀람({round(surprise_score*100)}%)\")\n",
    "    surprise = round(surprise_score*100)\n",
    "    \n",
    "surprise_predict(\"원재상\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tender-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259374\n",
      "20799\n",
      "[('하다/Verb', 8087), ('이/Josa', 4939), ('./Punctuation', 4679), ('가/Josa', 3509), ('?/Punctuation', 3201), ('에/Josa', 3168), ('들/Suffix', 3097), ('../Punctuation', 2820), ('을/Josa', 2523), ('은/Josa', 2512)]\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 2s 38ms/step - loss: 0.4539 - binary_accuracy: 0.8403\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.3336 - binary_accuracy: 0.8584\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3007 - binary_accuracy: 0.8716\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2823 - binary_accuracy: 0.8799\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2669 - binary_accuracy: 0.8884\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2512 - binary_accuracy: 0.8969\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2341 - binary_accuracy: 0.9055\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2162 - binary_accuracy: 0.9152\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1975 - binary_accuracy: 0.9247\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1789 - binary_accuracy: 0.9337\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1604 - binary_accuracy: 0.9425\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1410 - binary_accuracy: 0.9493\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1232 - binary_accuracy: 0.9563\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1050 - binary_accuracy: 0.9651\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0885 - binary_accuracy: 0.9719\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0734 - binary_accuracy: 0.9770\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0604 - binary_accuracy: 0.9815\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0495 - binary_accuracy: 0.9853\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0407 - binary_accuracy: 0.9887\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0328 - binary_accuracy: 0.9908\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0263 - binary_accuracy: 0.9934\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0217 - binary_accuracy: 0.9939\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - binary_accuracy: 0.9952\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - binary_accuracy: 0.9959\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0126 - binary_accuracy: 0.9966\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0113 - binary_accuracy: 0.9967\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - binary_accuracy: 0.9973\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0083 - binary_accuracy: 0.9974\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0077 - binary_accuracy: 0.9974\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0070 - binary_accuracy: 0.9977A: 0s - loss: 0.0036 - binary_accuracy: \n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0066 - binary_accuracy: 0.9979\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0060 - binary_accuracy: 0.9980\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0061 - binary_accuracy: 0.9979\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0054 - binary_accuracy: 0.9981\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0054 - binary_accuracy: 0.9979\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0050 - binary_accuracy: 0.9981\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0052 - binary_accuracy: 0.9979\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0048 - binary_accuracy: 0.9980\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0047 - binary_accuracy: 0.9982\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0045 - binary_accuracy: 0.9982\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0050 - binary_accuracy: 0.9982\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0044 - binary_accuracy: 0.9984\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0044 - binary_accuracy: 0.9982\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0044 - binary_accuracy: 0.9982\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0044 - binary_accuracy: 0.9984\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0044 - binary_accuracy: 0.9981\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0043 - binary_accuracy: 0.9981\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0044 - binary_accuracy: 0.9981\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0043 - binary_accuracy: 0.9984\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0042 - binary_accuracy: 0.9983\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.9644 - binary_accuracy: 0.8136\n",
      "분노(18%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_anger_df = pd.read_csv(\"angertrainalldata.txt\",\"\\t\")\n",
    "test_anger_df = pd.read_csv(\"angertestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_anger = Okt()\n",
    "\n",
    "def anger_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_anger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_anger_df.isnull().any()\n",
    "train_anger_df['document'] = train_anger_df['document'].fillna('');\n",
    "train_anger_df.isnull().any()\n",
    "test_anger_df['document'] = test_anger_df['document'].fillna('');\n",
    "\n",
    "train_anger_docs = [(anger_tokenize(row[1]), row[2]) for row in train_anger_df.values]\n",
    "test_anger_docs = [(anger_tokenize(row[1]),row[2]) for row in test_anger_df.values]\n",
    "\n",
    "tokens = [t for d in train_anger_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "anger_text = nltk.Text(tokens, name='NMSC')\n",
    "print(len(anger_text.tokens))\n",
    "print(len(set(anger_text.tokens)))\n",
    "print(anger_text.vocab().most_common(10))\n",
    "\n",
    "ANGER_FREQUENCY_COUNT = 2000;\n",
    "anger_selected_words = [f[0] for f in anger_text.vocab().most_common(ANGER_FREQUENCY_COUNT)]\n",
    "\n",
    "def anger_term_frequency(doc):\n",
    "    return [doc.count(word) for word in anger_selected_words]\n",
    "\n",
    "x_anger_train = [anger_term_frequency(d) for d,_ in train_anger_docs]\n",
    "x_anger_test = [anger_term_frequency(d) for d,_ in test_anger_docs]\n",
    "y_anger_train = [c for _,c in train_anger_docs]\n",
    "y_anger_test = [c for _,c in test_anger_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_anger_train = np.asarray(x_anger_train).astype('float32')\n",
    "x_anger_test = np.asarray(x_anger_test). astype('float32')\n",
    "\n",
    "y_anger_train = np.asarray(y_anger_train).astype('float32')\n",
    "y_anger_test = np.asarray(y_anger_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "anger_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(ANGER_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "anger_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "anger_model.fit(x_anger_train, y_anger_train, epochs=50, batch_size=512)\n",
    "anger_results = anger_model.evaluate(x_anger_test, y_anger_test)\n",
    "\n",
    "anger_review = \"너무 화난다\"\n",
    "anger_token = anger_tokenize(anger_review)\n",
    "\n",
    "tf = anger_term_frequency(anger_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(anger_model.predict(data))\n",
    "\n",
    "def anger_predict(predict):\n",
    "    anger_token = anger_tokenize(predict)\n",
    "    anger_tfq = anger_term_frequency(anger_token)\n",
    "    anger_data = np.expand_dims(np.asarray(anger_tfq).astype('float32'), axis=0)\n",
    "    anger_score = float(anger_model.predict(anger_data))\n",
    "    print(f\"분노({round(anger_score*100)}%)\")\n",
    "    anger = round(anger_score*100)\n",
    "    \n",
    "anger_predict(\"분노\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accessory-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259374\n",
      "20799\n",
      "[('하다/Verb', 8087), ('이/Josa', 4939), ('./Punctuation', 4679), ('가/Josa', 3509), ('?/Punctuation', 3201), ('에/Josa', 3168), ('들/Suffix', 3097), ('../Punctuation', 2820), ('을/Josa', 2523), ('은/Josa', 2512)]\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 2s 37ms/step - loss: 0.4108 - binary_accuracy: 0.8581\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 1s 14ms/step - loss: 0.2798 - binary_accuracy: 0.8885\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2450 - binary_accuracy: 0.9040\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2272 - binary_accuracy: 0.9122\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2136 - binary_accuracy: 0.9169\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2010 - binary_accuracy: 0.9223\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1883 - binary_accuracy: 0.9278\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1751 - binary_accuracy: 0.9339\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1610 - binary_accuracy: 0.9401\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1465 - binary_accuracy: 0.9470\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1319 - binary_accuracy: 0.9533\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1179 - binary_accuracy: 0.9604\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1035 - binary_accuracy: 0.9658\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0887 - binary_accuracy: 0.9715\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0744 - binary_accuracy: 0.9773\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0621 - binary_accuracy: 0.9814\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0510 - binary_accuracy: 0.9850\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0413 - binary_accuracy: 0.9879\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0331 - binary_accuracy: 0.9904\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0267 - binary_accuracy: 0.9927\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0217 - binary_accuracy: 0.9943\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0181 - binary_accuracy: 0.9953\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0149 - binary_accuracy: 0.9960\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0125 - binary_accuracy: 0.9963\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - binary_accuracy: 0.9968\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0094 - binary_accuracy: 0.9971\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0084 - binary_accuracy: 0.9974\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0077 - binary_accuracy: 0.9976\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0070 - binary_accuracy: 0.9979\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0067 - binary_accuracy: 0.9978\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0061 - binary_accuracy: 0.9980\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0061 - binary_accuracy: 0.9981\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0058 - binary_accuracy: 0.9981\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0057 - binary_accuracy: 0.9982\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0055 - binary_accuracy: 0.9981\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0052 - binary_accuracy: 0.9982\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0054 - binary_accuracy: 0.9980\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0050 - binary_accuracy: 0.9982\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0049 - binary_accuracy: 0.9982\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0049 - binary_accuracy: 0.9981\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0051 - binary_accuracy: 0.9981\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0048 - binary_accuracy: 0.9983\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0047 - binary_accuracy: 0.9981\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0049 - binary_accuracy: 0.9983\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0048 - binary_accuracy: 0.9983\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0047 - binary_accuracy: 0.9983\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0047 - binary_accuracy: 0.9982\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0047 - binary_accuracy: 0.9983\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0047 - binary_accuracy: 0.9981\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0046 - binary_accuracy: 0.9982\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.5546 - binary_accuracy: 0.8640\n",
      "슬픔(2%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_sadness_df = pd.read_csv(\"sadnesstrainalldata.txt\",\"\\t\")\n",
    "test_sadness_df = pd.read_csv(\"sadnesstestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_sadness = Okt()\n",
    "\n",
    "def sadness_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_sadness.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_sadness_df.isnull().any()\n",
    "train_sadness_df['document'] = train_sadness_df['document'].fillna('');\n",
    "train_sadness_df.isnull().any()\n",
    "test_sadness_df['document'] = test_sadness_df['document'].fillna('');\n",
    "\n",
    "train_sadness_docs = [(sadness_tokenize(row[1]), row[2]) for row in train_sadness_df.values]\n",
    "test_sadness_docs = [(sadness_tokenize(row[1]),row[2]) for row in test_sadness_df.values]\n",
    "\n",
    "tokens = [t for d in train_sadness_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "sadness_text = nltk.Text(tokens, name='NMSC')\n",
    "print(len(sadness_text.tokens))\n",
    "print(len(set(sadness_text.tokens)))\n",
    "print(sadness_text.vocab().most_common(10))\n",
    "\n",
    "SADNESS_FREQUENCY_COUNT = 2000;\n",
    "sadness_selected_words = [f[0] for f in fear_text.vocab().most_common(SADNESS_FREQUENCY_COUNT)]\n",
    "\n",
    "def sadness_term_frequency(doc):\n",
    "    return [doc.count(word) for word in sadness_selected_words]\n",
    "\n",
    "x_sadness_train = [sadness_term_frequency(d) for d,_ in train_sadness_docs]\n",
    "x_sadness_test = [sadness_term_frequency(d) for d,_ in test_sadness_docs]\n",
    "y_sadness_train = [c for _,c in train_sadness_docs]\n",
    "y_sadness_test = [c for _,c in test_sadness_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_sadness_train = np.asarray(x_sadness_train).astype('float32')\n",
    "x_sadness_test = np.asarray(x_sadness_test). astype('float32')\n",
    "\n",
    "y_sadness_train = np.asarray(y_sadness_train).astype('float32')\n",
    "y_sadness_test = np.asarray(y_sadness_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "sadness_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(SADNESS_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "sadness_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "sadness_model.fit(x_sadness_train, y_sadness_train, epochs=50, batch_size=512)\n",
    "sadness_results = sadness_model.evaluate(x_sadness_test, y_sadness_test)\n",
    "sadness_review = \"너무 화난다\"\n",
    "sadness_token = sadness_tokenize(sadness_review)\n",
    "\n",
    "tf = sadness_term_frequency(sadness_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(sadness_model.predict(data))\n",
    "\n",
    "def sadness_predict(predict):\n",
    "    sadness_token = sadness_tokenize(predict)\n",
    "    sadness_tfq = sadness_term_frequency(sadness_token)\n",
    "    sadness_data = np.expand_dims(np.asarray(sadness_tfq).astype('float32'), axis=0)\n",
    "    sadness_score = float(sadness_model.predict(sadness_data))\n",
    "    print(f\"슬픔({round(sadness_score*100)}%)\")\n",
    "    sad = round(sadness_score*100)\n",
    "    \n",
    "sadness_predict(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "satellite-hopkins",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259374\n",
      "20799\n",
      "[('하다/Verb', 8087), ('이/Josa', 4939), ('./Punctuation', 4679), ('가/Josa', 3509), ('?/Punctuation', 3201), ('에/Josa', 3168), ('들/Suffix', 3097), ('../Punctuation', 2820), ('을/Josa', 2523), ('은/Josa', 2512)]\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 2s 37ms/step - loss: 0.4194 - binary_accuracy: 0.8687\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.3341 - binary_accuracy: 0.8748\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3092 - binary_accuracy: 0.8767\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2914 - binary_accuracy: 0.8836\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2745 - binary_accuracy: 0.8910\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2566 - binary_accuracy: 0.9001\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2371 - binary_accuracy: 0.9087\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2170 - binary_accuracy: 0.9170\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1967 - binary_accuracy: 0.9255\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1765 - binary_accuracy: 0.9345\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1579 - binary_accuracy: 0.9415\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1399 - binary_accuracy: 0.9488\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1225 - binary_accuracy: 0.9581\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1061 - binary_accuracy: 0.9643\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0919 - binary_accuracy: 0.9690\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0782 - binary_accuracy: 0.9750\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0669 - binary_accuracy: 0.9793\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0558 - binary_accuracy: 0.9822\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0490 - binary_accuracy: 0.9852\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0402 - binary_accuracy: 0.9884\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0337 - binary_accuracy: 0.9901\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0292 - binary_accuracy: 0.9924\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0239 - binary_accuracy: 0.9936\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0207 - binary_accuracy: 0.9947\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0173 - binary_accuracy: 0.9953\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0152 - binary_accuracy: 0.9957\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - binary_accuracy: 0.9964\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - binary_accuracy: 0.9963\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0111 - binary_accuracy: 0.9968\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0104 - binary_accuracy: 0.9967\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0092 - binary_accuracy: 0.9969\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0087 - binary_accuracy: 0.9970\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0082 - binary_accuracy: 0.9975\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0076 - binary_accuracy: 0.9976\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0074 - binary_accuracy: 0.9974\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0070 - binary_accuracy: 0.9975\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0067 - binary_accuracy: 0.9976\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0067 - binary_accuracy: 0.9978\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0063 - binary_accuracy: 0.9976\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0062 - binary_accuracy: 0.9981\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0059 - binary_accuracy: 0.9977\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0060 - binary_accuracy: 0.9980\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0058 - binary_accuracy: 0.9979\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0057 - binary_accuracy: 0.9980\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0056 - binary_accuracy: 0.9981\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0056 - binary_accuracy: 0.9978\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0054 - binary_accuracy: 0.9979\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0055 - binary_accuracy: 0.9980\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0054 - binary_accuracy: 0.9979\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0051 - binary_accuracy: 0.9979\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.7903 - binary_accuracy: 0.7989\n",
      "중립(0%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_neutral_df = pd.read_csv(\"neutraltrainalldata.txt\",\"\\t\")\n",
    "test_neutral_df = pd.read_csv(\"neutraltestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_neutral = Okt()\n",
    "\n",
    "def neutral_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_neutral.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_neutral_docs = [(neutral_tokenize(row[1]), row[2]) for row in train_neutral_df.values]\n",
    "test_neutral_docs = [(neutral_tokenize(row[1]),row[2]) for row in test_neutral_df.values]\n",
    "\n",
    "tokens = [t for d in train_neutral_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "neutral_text = nltk.Text(tokens, name='NMSC')\n",
    "print(len(neutral_text.tokens))\n",
    "print(len(set(neutral_text.tokens)))\n",
    "print(neutral_text.vocab().most_common(10))\n",
    "\n",
    "NEUTRAL_FREQUENCY_COUNT = 2000;\n",
    "neutral_selected_words = [f[0] for f in neutral_text.vocab().most_common(NEUTRAL_FREQUENCY_COUNT)]\n",
    "\n",
    "def neutral_term_frequency(doc):\n",
    "    return [doc.count(word) for word in neutral_selected_words]\n",
    "\n",
    "x_neutral_train = [neutral_term_frequency(d) for d,_ in train_neutral_docs]\n",
    "x_neutral_test = [neutral_term_frequency(d) for d,_ in test_neutral_docs]\n",
    "y_neutral_train = [c for _,c in train_neutral_docs]\n",
    "y_neutral_test = [c for _,c in test_neutral_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_neutral_train = np.asarray(x_neutral_train).astype('float32')\n",
    "x_neutral_test = np.asarray(x_neutral_test). astype('float32')\n",
    "\n",
    "y_neutral_train = np.asarray(y_neutral_train).astype('float32')\n",
    "y_neutral_test = np.asarray(y_neutral_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "neutral_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(NEUTRAL_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "neutral_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "neutral_model.fit(x_neutral_train, y_neutral_train, epochs=50, batch_size=512)\n",
    "neutral_results = neutral_model.evaluate(x_neutral_test, y_neutral_test)\n",
    "neutral_review = \"너무 화난다\"\n",
    "neutral_token = neutral_tokenize(neutral_review)\n",
    "\n",
    "tf = neutral_term_frequency(neutral_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(neutral_model.predict(data))\n",
    "\n",
    "def neutral_predict(predict):\n",
    "    neutral_token = neutral_tokenize(predict)\n",
    "    neutral_tfq = neutral_term_frequency(neutral_token)\n",
    "    neutral_data = np.expand_dims(np.asarray(neutral_tfq).astype('float32'), axis=0)\n",
    "    neutral_score = float(neutral_model.predict(neutral_data))\n",
    "    print(f\"중립({round(neutral_score*100)}%)\")\n",
    "    neutral = round(neutral_score*100)\n",
    "\n",
    "neutral_predict(\"아무 생각이 없다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ancient-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259374\n",
      "20799\n",
      "[('하다/Verb', 8087), ('이/Josa', 4939), ('./Punctuation', 4679), ('가/Josa', 3509), ('?/Punctuation', 3201), ('에/Josa', 3168), ('들/Suffix', 3097), ('../Punctuation', 2820), ('을/Josa', 2523), ('은/Josa', 2512)]\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 2s 38ms/step - loss: 0.4079 - binary_accuracy: 0.8400\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.2260 - binary_accuracy: 0.9207\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1845 - binary_accuracy: 0.9334\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1687 - binary_accuracy: 0.9377\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1578 - binary_accuracy: 0.9414\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1495 - binary_accuracy: 0.9446\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1413 - binary_accuracy: 0.9480\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1334 - binary_accuracy: 0.9512\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1255 - binary_accuracy: 0.9546\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1173 - binary_accuracy: 0.9579\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1091 - binary_accuracy: 0.9607\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1011 - binary_accuracy: 0.9640\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0924 - binary_accuracy: 0.9675\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0844 - binary_accuracy: 0.9700\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0763 - binary_accuracy: 0.9740\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0688 - binary_accuracy: 0.9770\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0617 - binary_accuracy: 0.9793\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0543 - binary_accuracy: 0.9824\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0479 - binary_accuracy: 0.9844\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0411 - binary_accuracy: 0.9872\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0358 - binary_accuracy: 0.9887\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0306 - binary_accuracy: 0.9902\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0260 - binary_accuracy: 0.9922\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0227 - binary_accuracy: 0.9927\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0195 - binary_accuracy: 0.9939\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0171 - binary_accuracy: 0.9947\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0150 - binary_accuracy: 0.9953\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0133 - binary_accuracy: 0.9958\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0119 - binary_accuracy: 0.9963\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0106 - binary_accuracy: 0.9968\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0099 - binary_accuracy: 0.9967\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - binary_accuracy: 0.9971\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0084 - binary_accuracy: 0.9970\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0079 - binary_accuracy: 0.9973\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0075 - binary_accuracy: 0.9975\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0070 - binary_accuracy: 0.9977\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0068 - binary_accuracy: 0.9976\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0065 - binary_accuracy: 0.9976\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0064 - binary_accuracy: 0.9977\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0061 - binary_accuracy: 0.9976\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0060 - binary_accuracy: 0.9977\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0059 - binary_accuracy: 0.9979\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0057 - binary_accuracy: 0.9978\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0058 - binary_accuracy: 0.9979\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0059 - binary_accuracy: 0.9979\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0055 - binary_accuracy: 0.9978\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0055 - binary_accuracy: 0.9979\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0053 - binary_accuracy: 0.9980\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0054 - binary_accuracy: 0.9977\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0052 - binary_accuracy: 0.9981\n",
      "362/362 [==============================] - 0s 1ms/step - loss: 1.4809 - binary_accuracy: 0.8835\n",
      "행복(3%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_happy_df = pd.read_csv(\"happytrainalldata.txt\",\"\\t\")\n",
    "test_happy_df = pd.read_csv(\"happytestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_happy = Okt()\n",
    "\n",
    "def happy_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_happy.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_happy_docs = [(happy_tokenize(row[1]), row[2]) for row in train_happy_df.values]\n",
    "test_happy_docs = [(happy_tokenize(row[1]),row[2]) for row in test_happy_df.values]\n",
    "\n",
    "tokens = [t for d in train_happy_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "happy_text = nltk.Text(tokens, name='NMSC')\n",
    "print(len(happy_text.tokens))\n",
    "print(len(set(happy_text.tokens)))\n",
    "print(happy_text.vocab().most_common(10))\n",
    "\n",
    "HAPPY_FREQUENCY_COUNT = 2000;\n",
    "happy_selected_words = [f[0] for f in happy_text.vocab().most_common(HAPPY_FREQUENCY_COUNT)]\n",
    "\n",
    "def happy_term_frequency(doc):\n",
    "    return [doc.count(word) for word in happy_selected_words]\n",
    "\n",
    "x_happy_train = [happy_term_frequency(d) for d,_ in train_happy_docs]\n",
    "x_happy_test = [happy_term_frequency(d) for d,_ in test_happy_docs]\n",
    "y_happy_train = [c for _,c in train_happy_docs]\n",
    "y_happy_test = [c for _,c in test_happy_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_happy_train = np.asarray(x_happy_train).astype('float32')\n",
    "x_happy_test = np.asarray(x_happy_test). astype('float32')\n",
    "\n",
    "y_happy_train = np.asarray(y_happy_train).astype('float32')\n",
    "y_happy_test = np.asarray(y_happy_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "happy_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(HAPPY_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "happy_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "happy_model.fit(x_happy_train, y_happy_train, epochs=50, batch_size=512)\n",
    "happy_results = happy_model.evaluate(x_happy_test, y_happy_test)\n",
    "\n",
    "happy_review = \"너무 화난다\"\n",
    "happy_token = happy_tokenize(happy_review)\n",
    "\n",
    "tf = happy_term_frequency(happy_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(happy_model.predict(data))\n",
    "\n",
    "def happy_predict(predict):\n",
    "    happy_token = happy_tokenize(predict)\n",
    "    happy_tfq = happy_term_frequency(happy_token)\n",
    "    happy_data = np.expand_dims(np.asarray(happy_tfq).astype('float32'), axis=0)\n",
    "    happy_score = float(happy_model.predict(happy_data))\n",
    "    print(f\"행복({round(happy_score*100)}%)\")\n",
    "    happy = round(happy_score*100)\n",
    "\n",
    "happy_predict(\"너무해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "packed-parcel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259374\n",
      "20799\n",
      "[('하다/Verb', 8087), ('이/Josa', 4939), ('./Punctuation', 4679), ('가/Josa', 3509), ('?/Punctuation', 3201), ('에/Josa', 3168), ('들/Suffix', 3097), ('../Punctuation', 2820), ('을/Josa', 2523), ('은/Josa', 2512)]\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 2s 37ms/step - loss: 0.4397 - binary_accuracy: 0.8552\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.3523 - binary_accuracy: 0.8596\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3246 - binary_accuracy: 0.8636\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3061 - binary_accuracy: 0.8707\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2890 - binary_accuracy: 0.8793\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2711 - binary_accuracy: 0.8897\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2520 - binary_accuracy: 0.9006\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2317 - binary_accuracy: 0.9104\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2097 - binary_accuracy: 0.9206\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1869 - binary_accuracy: 0.9322\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1629 - binary_accuracy: 0.9434\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1391 - binary_accuracy: 0.9530\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1182 - binary_accuracy: 0.9616\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0968 - binary_accuracy: 0.9701\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0790 - binary_accuracy: 0.9766\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0630 - binary_accuracy: 0.9819\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0499 - binary_accuracy: 0.9863\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0392 - binary_accuracy: 0.9898\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0312 - binary_accuracy: 0.9921\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0255 - binary_accuracy: 0.9937\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0195 - binary_accuracy: 0.9954\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0157 - binary_accuracy: 0.9961\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0129 - binary_accuracy: 0.9968\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0106 - binary_accuracy: 0.9971\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0089 - binary_accuracy: 0.9977\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0080 - binary_accuracy: 0.9978\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0069 - binary_accuracy: 0.9977\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0061 - binary_accuracy: 0.9983\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0055 - binary_accuracy: 0.9984\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0050 - binary_accuracy: 0.9985\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0046 - binary_accuracy: 0.9985\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0046 - binary_accuracy: 0.9983\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0047 - binary_accuracy: 0.9985\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0041 - binary_accuracy: 0.9987\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0042 - binary_accuracy: 0.9986\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0041 - binary_accuracy: 0.9986\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0040 - binary_accuracy: 0.9985\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0040 - binary_accuracy: 0.9985\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0037 - binary_accuracy: 0.9986\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0040 - binary_accuracy: 0.9986\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0037 - binary_accuracy: 0.9986\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0037 - binary_accuracy: 0.9986\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0038 - binary_accuracy: 0.9985\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0038 - binary_accuracy: 0.9986\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0036 - binary_accuracy: 0.9987\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0035 - binary_accuracy: 0.9986\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0034 - binary_accuracy: 0.9987\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0034 - binary_accuracy: 0.9986\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0034 - binary_accuracy: 0.9986\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0034 - binary_accuracy: 0.9987\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 2.9069 - binary_accuracy: 0.7540\n",
      "혐오(0%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_disgust_df = pd.read_csv(\"disgusttrainalldata.txt\",\"\\t\")\n",
    "test_disgust_df = pd.read_csv(\"disgusttestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_disgust = Okt()\n",
    "\n",
    "def disgust_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_disgust.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_disgust_docs = [(disgust_tokenize(row[1]), row[2]) for row in train_disgust_df.values]\n",
    "test_disgust_docs = [(disgust_tokenize(row[1]),row[2]) for row in test_disgust_df.values]\n",
    "\n",
    "tokens = [t for d in train_disgust_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "disgust_text = nltk.Text(tokens, name='NMSC')\n",
    "print(len(disgust_text.tokens))\n",
    "print(len(set(disgust_text.tokens)))\n",
    "print(disgust_text.vocab().most_common(10))\n",
    "\n",
    "DISGUST_FREQUENCY_COUNT = 2000;\n",
    "disgust_selected_words = [f[0] for f in disgust_text.vocab().most_common(DISGUST_FREQUENCY_COUNT)]\n",
    "\n",
    "def disgust_term_frequency(doc):\n",
    "    return [doc.count(word) for word in disgust_selected_words]\n",
    "\n",
    "x_disgust_train = [disgust_term_frequency(d) for d,_ in train_disgust_docs]\n",
    "x_disgust_test = [disgust_term_frequency(d) for d,_ in test_disgust_docs]\n",
    "y_disgust_train = [c for _,c in train_disgust_docs]\n",
    "y_disgust_test = [c for _,c in test_disgust_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_disgust_train = np.asarray(x_disgust_train).astype('float32')\n",
    "x_disgust_test = np.asarray(x_disgust_test). astype('float32')\n",
    "\n",
    "y_disgust_train = np.asarray(y_disgust_train).astype('float32')\n",
    "y_disgust_test = np.asarray(y_disgust_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "disgust_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(DISGUST_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "disgust_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "disgust_model.fit(x_disgust_train, y_disgust_train, epochs=50, batch_size=512)\n",
    "disgust_results = neutral_model.evaluate(x_disgust_test, y_disgust_test)\n",
    "disgust_review = \"너무 화난다\"\n",
    "disgust_token = disgust_tokenize(disgust_review)\n",
    "\n",
    "tf = disgust_term_frequency(disgust_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(disgust_model.predict(data))\n",
    "\n",
    "def disgust_predict(predict):\n",
    "    disgust_token = disgust_tokenize(predict)\n",
    "    disgust_tfq = disgust_term_frequency(disgust_token)\n",
    "    disgust_data = np.expand_dims(np.asarray(disgust_tfq).astype('float32'), axis=0)\n",
    "    disgust_score = float(disgust_model.predict(disgust_data))\n",
    "    print(f\"혐오({round(disgust_score*100)}%)\")\n",
    "    disgust = round(disgust_score*100)\n",
    "    \n",
    "disgust_predict(\"원재상\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-punishment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
