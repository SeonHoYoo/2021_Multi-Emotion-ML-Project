{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abroad-gathering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4166 - binary_accuracy: 0.8568\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3167 - binary_accuracy: 0.8683\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2842 - binary_accuracy: 0.8833\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2645 - binary_accuracy: 0.8933\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2476 - binary_accuracy: 0.9012\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2319 - binary_accuracy: 0.9078\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2155 - binary_accuracy: 0.9165\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1988 - binary_accuracy: 0.9233\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1818 - binary_accuracy: 0.9318\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1639 - binary_accuracy: 0.9394\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1468 - binary_accuracy: 0.9474\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1295 - binary_accuracy: 0.9544\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1128 - binary_accuracy: 0.9621\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0975 - binary_accuracy: 0.9676\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0833 - binary_accuracy: 0.9728\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0713 - binary_accuracy: 0.9770\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0604 - binary_accuracy: 0.9803\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0511 - binary_accuracy: 0.9836\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0435 - binary_accuracy: 0.9859\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0379 - binary_accuracy: 0.9876\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0331 - binary_accuracy: 0.9890\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0289 - binary_accuracy: 0.9899\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0259 - binary_accuracy: 0.9907\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0236 - binary_accuracy: 0.9915\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0213 - binary_accuracy: 0.9926\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0195 - binary_accuracy: 0.9925\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0187 - binary_accuracy: 0.9927\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0171 - binary_accuracy: 0.9937\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0162 - binary_accuracy: 0.9940\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0157 - binary_accuracy: 0.9939\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0153 - binary_accuracy: 0.9939\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0144 - binary_accuracy: 0.9944\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0141 - binary_accuracy: 0.9947\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0141 - binary_accuracy: 0.9944\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0135 - binary_accuracy: 0.9944\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0133 - binary_accuracy: 0.9947\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0131 - binary_accuracy: 0.9947\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0128 - binary_accuracy: 0.9947\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0127 - binary_accuracy: 0.9949\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - binary_accuracy: 0.9949\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0124 - binary_accuracy: 0.9949\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - binary_accuracy: 0.9945\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0123 - binary_accuracy: 0.9947\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0119 - binary_accuracy: 0.9953\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0121 - binary_accuracy: 0.9949\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0118 - binary_accuracy: 0.9949\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0120 - binary_accuracy: 0.9949\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0114 - binary_accuracy: 0.9954\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0120 - binary_accuracy: 0.9947\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - binary_accuracy: 0.9951\n",
      "362/362 [==============================] - 0s 750us/step - loss: 1.7733 - binary_accuracy: 0.8358\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.4686 - binary_accuracy: 0.8286\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.3541 - binary_accuracy: 0.8499\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.3168 - binary_accuracy: 0.8660\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.2947 - binary_accuracy: 0.8749\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2767 - binary_accuracy: 0.8837\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2583 - binary_accuracy: 0.8930\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2392 - binary_accuracy: 0.9019\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2197 - binary_accuracy: 0.9116\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1999 - binary_accuracy: 0.9208\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1800 - binary_accuracy: 0.9301\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1603 - binary_accuracy: 0.9400\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1402 - binary_accuracy: 0.9487\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1201 - binary_accuracy: 0.9575\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1016 - binary_accuracy: 0.9645\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0853 - binary_accuracy: 0.9703\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0722 - binary_accuracy: 0.9756\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0615 - binary_accuracy: 0.9796\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0518 - binary_accuracy: 0.9829\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0441 - binary_accuracy: 0.9849\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0380 - binary_accuracy: 0.9872\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0335 - binary_accuracy: 0.9888\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0294 - binary_accuracy: 0.9893\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0265 - binary_accuracy: 0.9905\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0239 - binary_accuracy: 0.9910\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0224 - binary_accuracy: 0.9918\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0204 - binary_accuracy: 0.9924\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0191 - binary_accuracy: 0.9928\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0178 - binary_accuracy: 0.9933\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0170 - binary_accuracy: 0.9938\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0163 - binary_accuracy: 0.9936\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0159 - binary_accuracy: 0.9939\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0152 - binary_accuracy: 0.9940\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0144 - binary_accuracy: 0.9942\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0146 - binary_accuracy: 0.9942\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0142 - binary_accuracy: 0.9943\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0138 - binary_accuracy: 0.9944\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0138 - binary_accuracy: 0.9943\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0136 - binary_accuracy: 0.9948\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0129 - binary_accuracy: 0.9946\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0131 - binary_accuracy: 0.9944\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0131 - binary_accuracy: 0.9944\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0128 - binary_accuracy: 0.9947\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0127 - binary_accuracy: 0.9947\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - binary_accuracy: 0.9951\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0122 - binary_accuracy: 0.9945\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0123 - binary_accuracy: 0.9950\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0122 - binary_accuracy: 0.9948\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0122 - binary_accuracy: 0.9948\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0119 - binary_accuracy: 0.9949\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0119 - binary_accuracy: 0.9949\n",
      "362/362 [==============================] - 0s 771us/step - loss: 2.1987 - binary_accuracy: 0.8109\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4394 - binary_accuracy: 0.8452\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3212 - binary_accuracy: 0.8636\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2882 - binary_accuracy: 0.8783\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2665 - binary_accuracy: 0.8888\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2461 - binary_accuracy: 0.8994\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2266 - binary_accuracy: 0.9102\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2063 - binary_accuracy: 0.9190\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1855 - binary_accuracy: 0.9290\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1645 - binary_accuracy: 0.9381\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1452 - binary_accuracy: 0.9473\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.1267 - binary_accuracy: 0.9553\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1090 - binary_accuracy: 0.9625\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0933 - binary_accuracy: 0.9683\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0786 - binary_accuracy: 0.9741\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0657 - binary_accuracy: 0.9790\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0545 - binary_accuracy: 0.9829\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0449 - binary_accuracy: 0.9864\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0371 - binary_accuracy: 0.9891\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0304 - binary_accuracy: 0.9914\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0251 - binary_accuracy: 0.9929\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0213 - binary_accuracy: 0.9938\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0180 - binary_accuracy: 0.9952\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0154 - binary_accuracy: 0.9954\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0135 - binary_accuracy: 0.9959\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0123 - binary_accuracy: 0.9964\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0107 - binary_accuracy: 0.9966\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0101 - binary_accuracy: 0.9970\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0095 - binary_accuracy: 0.9967\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0088 - binary_accuracy: 0.9970\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0084 - binary_accuracy: 0.9971\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0080 - binary_accuracy: 0.9971\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0081 - binary_accuracy: 0.9971\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0077 - binary_accuracy: 0.9974\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0073 - binary_accuracy: 0.9974\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0073 - binary_accuracy: 0.9975\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0072 - binary_accuracy: 0.9974\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0071 - binary_accuracy: 0.9973\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0071 - binary_accuracy: 0.9973\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0069 - binary_accuracy: 0.9974\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0070 - binary_accuracy: 0.9974\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0071 - binary_accuracy: 0.9976\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0067 - binary_accuracy: 0.9974\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0068 - binary_accuracy: 0.9974\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0066 - binary_accuracy: 0.9973\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0064 - binary_accuracy: 0.9977\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0068 - binary_accuracy: 0.9976\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0064 - binary_accuracy: 0.9975\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0067 - binary_accuracy: 0.9975\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0064 - binary_accuracy: 0.9976\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0066 - binary_accuracy: 0.9975\n",
      "362/362 [==============================] - 0s 742us/step - loss: 2.3193 - binary_accuracy: 0.8101\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4227 - binary_accuracy: 0.8523\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2935 - binary_accuracy: 0.8790\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2548 - binary_accuracy: 0.8987\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2332 - binary_accuracy: 0.9078\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2147 - binary_accuracy: 0.9157\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1978 - binary_accuracy: 0.9213\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1809 - binary_accuracy: 0.9295\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1639 - binary_accuracy: 0.9384\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1473 - binary_accuracy: 0.9459\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1311 - binary_accuracy: 0.9528\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1156 - binary_accuracy: 0.9596\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1005 - binary_accuracy: 0.9656\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0863 - binary_accuracy: 0.9705\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0733 - binary_accuracy: 0.9756\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0614 - binary_accuracy: 0.9791\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0514 - binary_accuracy: 0.9829\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0433 - binary_accuracy: 0.9856\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0372 - binary_accuracy: 0.9877\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0323 - binary_accuracy: 0.9891\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0282 - binary_accuracy: 0.9904\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0249 - binary_accuracy: 0.9915\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0224 - binary_accuracy: 0.9924\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0210 - binary_accuracy: 0.9926\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0189 - binary_accuracy: 0.9934\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0176 - binary_accuracy: 0.9937\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0166 - binary_accuracy: 0.9939\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0160 - binary_accuracy: 0.9940\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0152 - binary_accuracy: 0.9944\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0147 - binary_accuracy: 0.9946\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0140 - binary_accuracy: 0.9949\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0134 - binary_accuracy: 0.9951\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0132 - binary_accuracy: 0.9952\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0128 - binary_accuracy: 0.9950\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0125 - binary_accuracy: 0.9951\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - binary_accuracy: 0.9951\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0123 - binary_accuracy: 0.9950\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0121 - binary_accuracy: 0.9950\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0120 - binary_accuracy: 0.9953\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0119 - binary_accuracy: 0.9950\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0117 - binary_accuracy: 0.9953\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - binary_accuracy: 0.9950\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0115 - binary_accuracy: 0.9954\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0114 - binary_accuracy: 0.9952\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0114 - binary_accuracy: 0.9949\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0110 - binary_accuracy: 0.9957\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0112 - binary_accuracy: 0.9954\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0110 - binary_accuracy: 0.9953\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0110 - binary_accuracy: 0.9955\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - binary_accuracy: 0.9956\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - binary_accuracy: 0.9952\n",
      "362/362 [==============================] - 0s 669us/step - loss: 1.5329 - binary_accuracy: 0.8443\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4419 - binary_accuracy: 0.8579\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3359 - binary_accuracy: 0.8748\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3062 - binary_accuracy: 0.8771\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2852 - binary_accuracy: 0.8848\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2655 - binary_accuracy: 0.8935\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2448 - binary_accuracy: 0.9029\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2227 - binary_accuracy: 0.9133\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2000 - binary_accuracy: 0.9243\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1772 - binary_accuracy: 0.9345\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1559 - binary_accuracy: 0.9429\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1343 - binary_accuracy: 0.9523\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1147 - binary_accuracy: 0.9606\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0966 - binary_accuracy: 0.9688\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0810 - binary_accuracy: 0.9746\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0672 - binary_accuracy: 0.9793\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0562 - binary_accuracy: 0.9834\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0473 - binary_accuracy: 0.9856\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0404 - binary_accuracy: 0.9873\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0344 - binary_accuracy: 0.9894\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0304 - binary_accuracy: 0.9907\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0264 - binary_accuracy: 0.9923\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0236 - binary_accuracy: 0.9929\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0215 - binary_accuracy: 0.9933\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0195 - binary_accuracy: 0.9940\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0176 - binary_accuracy: 0.9944\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0164 - binary_accuracy: 0.9948\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0152 - binary_accuracy: 0.9953\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0146 - binary_accuracy: 0.9953\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0141 - binary_accuracy: 0.9955\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0132 - binary_accuracy: 0.9958\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0127 - binary_accuracy: 0.9958\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0124 - binary_accuracy: 0.9959\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0121 - binary_accuracy: 0.9959\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0116 - binary_accuracy: 0.9960\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0110 - binary_accuracy: 0.9960\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0113 - binary_accuracy: 0.9961\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0108 - binary_accuracy: 0.9962\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0108 - binary_accuracy: 0.9963\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0103 - binary_accuracy: 0.9963\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0100 - binary_accuracy: 0.9963\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0101 - binary_accuracy: 0.9964\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0101 - binary_accuracy: 0.9964\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0099 - binary_accuracy: 0.9964\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0097 - binary_accuracy: 0.9965\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0098 - binary_accuracy: 0.9966\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0096 - binary_accuracy: 0.9965\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0095 - binary_accuracy: 0.9967\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0096 - binary_accuracy: 0.9965\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0095 - binary_accuracy: 0.9965\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0091 - binary_accuracy: 0.9965\n",
      "362/362 [==============================] - 0s 747us/step - loss: 1.8279 - binary_accuracy: 0.8054\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4145 - binary_accuracy: 0.8417\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2348 - binary_accuracy: 0.9162\n",
      "Epoch 3/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1943 - binary_accuracy: 0.9285\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1767 - binary_accuracy: 0.9346\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1650 - binary_accuracy: 0.9375\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1552 - binary_accuracy: 0.9413\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1463 - binary_accuracy: 0.9447\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1377 - binary_accuracy: 0.9480\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1290 - binary_accuracy: 0.9513\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1209 - binary_accuracy: 0.9546\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1121 - binary_accuracy: 0.9578\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1037 - binary_accuracy: 0.9609\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0957 - binary_accuracy: 0.9645\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0874 - binary_accuracy: 0.9677\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0798 - binary_accuracy: 0.9713\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0725 - binary_accuracy: 0.9740\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0652 - binary_accuracy: 0.9767\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0588 - binary_accuracy: 0.9796\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0523 - binary_accuracy: 0.9816\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0463 - binary_accuracy: 0.9835\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0406 - binary_accuracy: 0.9859\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0356 - binary_accuracy: 0.9876\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0311 - binary_accuracy: 0.9890\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0275 - binary_accuracy: 0.9906\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0241 - binary_accuracy: 0.9917\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0214 - binary_accuracy: 0.9926\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0194 - binary_accuracy: 0.9932\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0179 - binary_accuracy: 0.9936\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0165 - binary_accuracy: 0.9940\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0157 - binary_accuracy: 0.9943\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0146 - binary_accuracy: 0.9944\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0140 - binary_accuracy: 0.9947\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0134 - binary_accuracy: 0.9950\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0128 - binary_accuracy: 0.9950\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0122 - binary_accuracy: 0.9956\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0121 - binary_accuracy: 0.9951\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0117 - binary_accuracy: 0.9955\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0115 - binary_accuracy: 0.9953\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0109 - binary_accuracy: 0.9959\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0110 - binary_accuracy: 0.9956\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0109 - binary_accuracy: 0.9955\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0106 - binary_accuracy: 0.9958\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0106 - binary_accuracy: 0.9959\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0106 - binary_accuracy: 0.9959\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0102 - binary_accuracy: 0.9959\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0101 - binary_accuracy: 0.9961\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0103 - binary_accuracy: 0.9957\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0102 - binary_accuracy: 0.9960\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0098 - binary_accuracy: 0.9959\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0102 - binary_accuracy: 0.9960\n",
      "362/362 [==============================] - 0s 709us/step - loss: 1.4095 - binary_accuracy: 0.8700\n",
      "Epoch 1/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.4512 - binary_accuracy: 0.8531\n",
      "Epoch 2/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3516 - binary_accuracy: 0.8596\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3202 - binary_accuracy: 0.8658\n",
      "Epoch 4/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2974 - binary_accuracy: 0.8771\n",
      "Epoch 5/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2752 - binary_accuracy: 0.8884\n",
      "Epoch 6/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2535 - binary_accuracy: 0.9012\n",
      "Epoch 7/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2315 - binary_accuracy: 0.9129\n",
      "Epoch 8/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2082 - binary_accuracy: 0.9237\n",
      "Epoch 9/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1845 - binary_accuracy: 0.9336\n",
      "Epoch 10/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1625 - binary_accuracy: 0.9445\n",
      "Epoch 11/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1398 - binary_accuracy: 0.9534\n",
      "Epoch 12/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1194 - binary_accuracy: 0.9612\n",
      "Epoch 13/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0999 - binary_accuracy: 0.9695\n",
      "Epoch 14/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0835 - binary_accuracy: 0.9762\n",
      "Epoch 15/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0670 - binary_accuracy: 0.9806\n",
      "Epoch 16/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0556 - binary_accuracy: 0.9852\n",
      "Epoch 17/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0436 - binary_accuracy: 0.9876\n",
      "Epoch 18/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0362 - binary_accuracy: 0.9903\n",
      "Epoch 19/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0291 - binary_accuracy: 0.9917\n",
      "Epoch 20/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0233 - binary_accuracy: 0.9934\n",
      "Epoch 21/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0193 - binary_accuracy: 0.9947\n",
      "Epoch 22/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0177 - binary_accuracy: 0.9952\n",
      "Epoch 23/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0143 - binary_accuracy: 0.9958\n",
      "Epoch 24/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0124 - binary_accuracy: 0.9966\n",
      "Epoch 25/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0109 - binary_accuracy: 0.9969\n",
      "Epoch 26/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0108 - binary_accuracy: 0.9970\n",
      "Epoch 27/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0090 - binary_accuracy: 0.9970\n",
      "Epoch 28/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0085 - binary_accuracy: 0.9973\n",
      "Epoch 29/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0081 - binary_accuracy: 0.9974\n",
      "Epoch 30/50\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0074 - binary_accuracy: 0.9976\n",
      "Epoch 31/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0072 - binary_accuracy: 0.9979\n",
      "Epoch 32/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0090 - binary_accuracy: 0.9978\n",
      "Epoch 33/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0067 - binary_accuracy: 0.9978\n",
      "Epoch 34/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0065 - binary_accuracy: 0.9979\n",
      "Epoch 35/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0085 - binary_accuracy: 0.9978\n",
      "Epoch 36/50\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0062 - binary_accuracy: 0.9981\n",
      "Epoch 37/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0061 - binary_accuracy: 0.9980\n",
      "Epoch 38/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0060 - binary_accuracy: 0.9981\n",
      "Epoch 39/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0059 - binary_accuracy: 0.9981\n",
      "Epoch 40/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0061 - binary_accuracy: 0.9981\n",
      "Epoch 41/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0079 - binary_accuracy: 0.9982\n",
      "Epoch 42/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0059 - binary_accuracy: 0.9980\n",
      "Epoch 43/50\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0058 - binary_accuracy: 0.9980\n",
      "Epoch 44/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0058 - binary_accuracy: 0.9981\n",
      "Epoch 45/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0071 - binary_accuracy: 0.9980\n",
      "Epoch 46/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0056 - binary_accuracy: 0.9980\n",
      "Epoch 47/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0055 - binary_accuracy: 0.9980\n",
      "Epoch 48/50\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0056 - binary_accuracy: 0.9981\n",
      "Epoch 49/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0056 - binary_accuracy: 0.9982\n",
      "Epoch 50/50\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0066 - binary_accuracy: 0.9980\n",
      "362/362 [==============================] - 0s 692us/step - loss: 3.0154 - binary_accuracy: 0.7657\n"
     ]
    }
   ],
   "source": [
    "#fear emotion\n",
    "\n",
    "#데이터 pandas로 읽기, 구문이 탭으로 구분되어있음.\n",
    "import pandas as pd\n",
    "train_fear_df = pd.read_csv(\"feartrainalldata.txt\",\"\\t\") \n",
    "test_fear_df = pd.read_csv(\"feartestalldata.txt\",\"\\t\")\n",
    "\n",
    "#konlpy는 띄어쓰기 알고리즘과 정규화를 이용해 맞춤법이 틀린 문장을 고치고 형태소 분석과 품사를 태깅해주는 클래스를 제공함.\n",
    "from konlpy.tag import Okt\n",
    "okt_fear = Okt()\n",
    "\n",
    "def fear_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_fear.pos(doc, norm=True, stem=False)]#08.22 stem=false로 바꿔봄. \n",
    "    #'구분자'.join(list) => 리스트 값과 값 사이에 구분자를 넣어 문자열을 하나로 합침. ex) '_'.join(['a','b','c'])이면 \"a_b_c\"로 반환\n",
    "    #pos(text) => 품사를 태깅한 상태로 명사를 변환, ex) 컴퓨터(x), 컵퓨터,noun(o)\n",
    "    #norm => 정규화 ex) 그래욬ㅋㅋㅋ => 그래요\n",
    "    #stem => 근어로 표현 ex) 그래요 => 그렇다\n",
    "\n",
    "train_fear_df.isnull().any()\n",
    "train_fear_df['document'] = train_fear_df['document'].fillna(''); #null값을 ''로 대체\n",
    "test_fear_df.isnull().any()\n",
    "test_fear_df['document'] = test_fear_df['document'].fillna('');\n",
    "\n",
    "train_fear_docs = [(fear_tokenize(row[1]), row[2]) for row in train_fear_df.values] #traindata 저장 row[1] : document / row[2] : label\n",
    "test_fear_docs = [(fear_tokenize(row[1]),row[2]) for row in test_fear_df.values] #testdata 저장\n",
    "\n",
    "tokens = [t for d in train_fear_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "fear_text = nltk.Text(tokens, name='FEAR') \n",
    "#문서 하나를 편리하게 탐색할 수 있는 기능 제공 (vocab().most_common() 사용)\n",
    "# print(len(fear_text.tokens)) #토큰 개수\n",
    "# print(len(set(fear_text.tokens))) #중복을 제외한 토큰 수\n",
    "# print(fear_text.vocab().most_common(10)) #출력빈도가 높은 상위 토큰 10개\n",
    "\n",
    "#countvectorization\n",
    "FEAR_FREQUENCY_COUNT = 3000; #자주 사용되는 토큰 설정\n",
    "fear_selected_words = [f[0] for f in fear_text.vocab().most_common(FEAR_FREQUENCY_COUNT)] #선택되어진 토큰들\n",
    "\n",
    "def fear_term_frequency(doc):\n",
    "    return [doc.count(word) for word in fear_selected_words]\n",
    "\n",
    "x_fear_train = [fear_term_frequency(d) for d,_ in train_fear_docs]\n",
    "x_fear_test = [fear_term_frequency(d) for d,_ in test_fear_docs]\n",
    "#x축에는 문서에 들어가는 단어 개수(단어들의 빈도수 정보)\n",
    "\n",
    "y_fear_train = [c for _,c in train_fear_docs]\n",
    "y_fear_test = [c for _,c in test_fear_docs]\n",
    "#y축에는 1 or 0, 분류 결과\n",
    "\n",
    "import numpy as np\n",
    "x_fear_train = np.asarray(x_fear_train).astype('float32')\n",
    "x_fear_test = np.asarray(x_fear_test). astype('float32')\n",
    "\n",
    "y_fear_train = np.asarray(y_fear_train).astype('float32')\n",
    "y_fear_test = np.asarray(y_fear_test).astype('float32')\n",
    "#np.asarray는 np.array와 달리 데이터 형태가 같을 때 복사하지 않음.\n",
    "#데이터 float로 형 변환\n",
    "\n",
    "import tensorflow as tf #텐서플로 케라스\n",
    "fear_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(FEAR_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "#레이어 구성은 dense층은 64개의 유닛, 활성함수는 relu, 마지막 층은 sigmoid 활성화 함수 사용\n",
    "\n",
    "fear_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "#손실 함수는 binary_crossentropy, RMSprop 옵티마이저를 통해 경사하강법 진행\n",
    "\n",
    "fear_model.fit(x_fear_train, y_fear_train, epochs=50, batch_size=512)\n",
    "#배치 사이즈 줄이면 한 번에 판단하는 데이터 수 증가함. 에포크 50번\n",
    "fear_results = fear_model.evaluate(x_fear_test, y_fear_test)\n",
    "\n",
    "fear_review = \"너무 무섭다\"\n",
    "fear_token = fear_tokenize(fear_review)\n",
    "\n",
    "fear_tf = fear_term_frequency(fear_token)\n",
    "f_data = np.expand_dims(np.asarray(fear_tf).astype('float32'),axis=0)\n",
    "float(fear_model.predict(f_data))\n",
    "\n",
    "#surprise emotion\n",
    "\n",
    "import pandas as pd\n",
    "train_surprise_df = pd.read_csv(\"surprisetrainalldata.txt\",\"\\t\")\n",
    "test_surprise_df = pd.read_csv(\"surprisetestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_surprise = Okt()\n",
    "\n",
    "def surprise_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_surprise.pos(doc, norm=True, stem=False)]\n",
    "\n",
    "train_surprise_df.isnull().any()\n",
    "train_surprise_df['document'] = train_surprise_df['document'].fillna('');\n",
    "test_surprise_df.isnull().any()\n",
    "test_surprise_df['document'] = test_surprise_df['document'].fillna('');\n",
    "\n",
    "train_surprise_docs = [(surprise_tokenize(row[1]), row[2]) for row in train_surprise_df.values]\n",
    "test_surprise_docs = [(surprise_tokenize(row[1]),row[2]) for row in test_surprise_df.values]\n",
    "\n",
    "tokens = [t for d in train_surprise_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "surprise_text = nltk.Text(tokens, name='SURP')\n",
    "#print(len(surprise_text.tokens))\n",
    "#print(len(set(surprise_text.tokens)))\n",
    "#print(surprise_text.vocab().most_common(10))\n",
    "\n",
    "SURPRISE_FREQUENCY_COUNT = 3000; #variable\n",
    "surprise_selected_words = [f[0] for f in surprise_text.vocab().most_common(SURPRISE_FREQUENCY_COUNT)]\n",
    "\n",
    "def surprise_term_frequency(doc):\n",
    "    return [doc.count(word) for word in surprise_selected_words]\n",
    "\n",
    "x_surprise_train = [surprise_term_frequency(d) for d,_ in train_surprise_docs]\n",
    "x_surprise_test = [surprise_term_frequency(d) for d,_ in test_surprise_docs]\n",
    "y_surprise_train = [c for _,c in train_surprise_docs]\n",
    "y_surprise_test = [c for _,c in test_surprise_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_surprise_train = np.asarray(x_surprise_train).astype('float32')\n",
    "x_surprise_test = np.asarray(x_surprise_test). astype('float32')\n",
    "\n",
    "y_surprise_train = np.asarray(y_surprise_train).astype('float32')\n",
    "y_surprise_test = np.asarray(y_surprise_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "surprise_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(SURPRISE_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "surprise_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "surprise_model.fit(x_surprise_train, y_surprise_train, epochs=50, batch_size=512)\n",
    "surprise_results = surprise_model.evaluate(x_surprise_test, y_surprise_test)\n",
    "surprise_review = \"너무 놀랍다\"\n",
    "surprise_token = surprise_tokenize(surprise_review)\n",
    "\n",
    "tf = surprise_term_frequency(surprise_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(surprise_model.predict(data))\n",
    "\n",
    "#anger emotion\n",
    "\n",
    "import pandas as pd\n",
    "train_anger_df = pd.read_csv(\"angertrainalldata.txt\",\"\\t\")\n",
    "test_anger_df = pd.read_csv(\"angertestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_anger = Okt()\n",
    "\n",
    "def anger_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_anger.pos(doc, norm=True, stem=False)]\n",
    "\n",
    "train_anger_df.isnull().any()\n",
    "train_anger_df['document'] = train_anger_df['document'].fillna('');\n",
    "test_anger_df.isnull().any()\n",
    "test_anger_df['document'] = test_anger_df['document'].fillna('');\n",
    "\n",
    "train_anger_docs = [(anger_tokenize(row[1]), row[2]) for row in train_anger_df.values]\n",
    "test_anger_docs = [(anger_tokenize(row[1]),row[2]) for row in test_anger_df.values]\n",
    "\n",
    "tokens = [t for d in train_anger_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "anger_text = nltk.Text(tokens, name='ANGE')\n",
    "#print(len(anger_text.tokens))\n",
    "#print(len(set(anger_text.tokens)))\n",
    "#print(anger_text.vocab().most_common(10))\n",
    "\n",
    "ANGER_FREQUENCY_COUNT = 3000; #variable\n",
    "anger_selected_words = [f[0] for f in anger_text.vocab().most_common(ANGER_FREQUENCY_COUNT)]\n",
    "\n",
    "def anger_term_frequency(doc):\n",
    "    return [doc.count(word) for word in anger_selected_words]\n",
    "\n",
    "x_anger_train = [anger_term_frequency(d) for d,_ in train_anger_docs]\n",
    "x_anger_test = [anger_term_frequency(d) for d,_ in test_anger_docs]\n",
    "y_anger_train = [c for _,c in train_anger_docs]\n",
    "y_anger_test = [c for _,c in test_anger_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_anger_train = np.asarray(x_anger_train).astype('float32')\n",
    "x_anger_test = np.asarray(x_anger_test). astype('float32')\n",
    "\n",
    "y_anger_train = np.asarray(y_anger_train).astype('float32')\n",
    "y_anger_test = np.asarray(y_anger_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "anger_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(ANGER_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "anger_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "anger_model.fit(x_anger_train, y_anger_train, epochs=50, batch_size=512)\n",
    "anger_results = anger_model.evaluate(x_anger_test, y_anger_test)\n",
    "\n",
    "anger_review = \"너무 화난다\"\n",
    "anger_token = anger_tokenize(anger_review)\n",
    "\n",
    "tf = anger_term_frequency(anger_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(anger_model.predict(data))\n",
    "\n",
    "#sadness emotion\n",
    "\n",
    "import pandas as pd\n",
    "train_sadness_df = pd.read_csv(\"sadnesstrainalldata.txt\",\"\\t\")\n",
    "test_sadness_df = pd.read_csv(\"sadnesstestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_sadness = Okt()\n",
    "\n",
    "def sadness_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_sadness.pos(doc, norm=True, stem=False)]\n",
    "\n",
    "train_sadness_df.isnull().any()\n",
    "train_sadness_df['document'] = train_sadness_df['document'].fillna('');\n",
    "test_sadness_df.isnull().any()\n",
    "test_sadness_df['document'] = test_sadness_df['document'].fillna('');\n",
    "\n",
    "train_sadness_docs = [(sadness_tokenize(row[1]), row[2]) for row in train_sadness_df.values]\n",
    "test_sadness_docs = [(sadness_tokenize(row[1]),row[2]) for row in test_sadness_df.values]\n",
    "\n",
    "tokens = [t for d in train_sadness_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "sadness_text = nltk.Text(tokens, name='SADN')\n",
    "#print(len(sadness_text.tokens))\n",
    "#print(len(set(sadness_text.tokens)))\n",
    "#print(sadness_text.vocab().most_common(10))\n",
    "\n",
    "SADNESS_FREQUENCY_COUNT = 3000; #variable\n",
    "sadness_selected_words = [f[0] for f in fear_text.vocab().most_common(SADNESS_FREQUENCY_COUNT)]\n",
    "\n",
    "def sadness_term_frequency(doc):\n",
    "    return [doc.count(word) for word in sadness_selected_words]\n",
    "\n",
    "x_sadness_train = [sadness_term_frequency(d) for d,_ in train_sadness_docs]\n",
    "x_sadness_test = [sadness_term_frequency(d) for d,_ in test_sadness_docs]\n",
    "y_sadness_train = [c for _,c in train_sadness_docs]\n",
    "y_sadness_test = [c for _,c in test_sadness_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_sadness_train = np.asarray(x_sadness_train).astype('float32')\n",
    "x_sadness_test = np.asarray(x_sadness_test). astype('float32')\n",
    "\n",
    "y_sadness_train = np.asarray(y_sadness_train).astype('float32')\n",
    "y_sadness_test = np.asarray(y_sadness_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "sadness_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(SADNESS_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "sadness_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "sadness_model.fit(x_sadness_train, y_sadness_train, epochs=50, batch_size=512)\n",
    "sadness_results = sadness_model.evaluate(x_sadness_test, y_sadness_test)\n",
    "sadness_review = \"너무 화난다\"\n",
    "sadness_token = sadness_tokenize(sadness_review)\n",
    "\n",
    "tf = sadness_term_frequency(sadness_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(sadness_model.predict(data))\n",
    "\n",
    "#neutral emotion\n",
    "\n",
    "import pandas as pd\n",
    "train_neutral_df = pd.read_csv(\"neutraltrainalldata.txt\",\"\\t\")\n",
    "test_neutral_df = pd.read_csv(\"neutraltestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_neutral = Okt()\n",
    "\n",
    "def neutral_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_neutral.pos(doc, norm=True, stem=False)]\n",
    "\n",
    "train_neutral_df.isnull().any()\n",
    "train_neutral_df['document'] = train_neutral_df['document'].fillna('');\n",
    "test_neutral_df.isnull().any()\n",
    "test_neutral_df['document'] = test_neutral_df['document'].fillna('');\n",
    "\n",
    "train_neutral_docs = [(neutral_tokenize(row[1]), row[2]) for row in train_neutral_df.values]\n",
    "test_neutral_docs = [(neutral_tokenize(row[1]),row[2]) for row in test_neutral_df.values]\n",
    "\n",
    "tokens = [t for d in train_neutral_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "neutral_text = nltk.Text(tokens, name='NEUT')\n",
    "#print(len(neutral_text.tokens))\n",
    "#print(len(set(neutral_text.tokens)))\n",
    "#print(neutral_text.vocab().most_common(10))\n",
    "\n",
    "NEUTRAL_FREQUENCY_COUNT = 3000; #variable\n",
    "neutral_selected_words = [f[0] for f in neutral_text.vocab().most_common(NEUTRAL_FREQUENCY_COUNT)]\n",
    "\n",
    "def neutral_term_frequency(doc):\n",
    "    return [doc.count(word) for word in neutral_selected_words]\n",
    "\n",
    "x_neutral_train = [neutral_term_frequency(d) for d,_ in train_neutral_docs]\n",
    "x_neutral_test = [neutral_term_frequency(d) for d,_ in test_neutral_docs]\n",
    "y_neutral_train = [c for _,c in train_neutral_docs]\n",
    "y_neutral_test = [c for _,c in test_neutral_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_neutral_train = np.asarray(x_neutral_train).astype('float32')\n",
    "x_neutral_test = np.asarray(x_neutral_test). astype('float32')\n",
    "\n",
    "y_neutral_train = np.asarray(y_neutral_train).astype('float32')\n",
    "y_neutral_test = np.asarray(y_neutral_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "neutral_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(NEUTRAL_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "neutral_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "neutral_model.fit(x_neutral_train, y_neutral_train, epochs=50, batch_size=512)\n",
    "neutral_results = neutral_model.evaluate(x_neutral_test, y_neutral_test)\n",
    "neutral_review = \"너무 화난다\"\n",
    "neutral_token = neutral_tokenize(neutral_review)\n",
    "\n",
    "tf = neutral_term_frequency(neutral_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(neutral_model.predict(data))\n",
    "\n",
    "# happy emotion\n",
    "\n",
    "import pandas as pd\n",
    "train_happy_df = pd.read_csv(\"happytrainalldata.txt\",\"\\t\")\n",
    "test_happy_df = pd.read_csv(\"happytestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_happy = Okt()\n",
    "\n",
    "def happy_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_happy.pos(doc, norm=True, stem=False)]\n",
    "\n",
    "train_happy_df.isnull().any()\n",
    "train_happy_df['document'] = train_happy_df['document'].fillna('');\n",
    "test_happy_df.isnull().any()\n",
    "test_happy_df['document'] = test_happy_df['document'].fillna('');\n",
    "\n",
    "train_happy_docs = [(happy_tokenize(row[1]), row[2]) for row in train_happy_df.values]\n",
    "test_happy_docs = [(happy_tokenize(row[1]),row[2]) for row in test_happy_df.values]\n",
    "\n",
    "tokens = [t for d in train_happy_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "happy_text = nltk.Text(tokens, name='HAPP')\n",
    "#print(len(happy_text.tokens))\n",
    "#print(len(set(happy_text.tokens)))\n",
    "#print(happy_text.vocab().most_common(10))\n",
    "\n",
    "HAPPY_FREQUENCY_COUNT = 3000; #variable\n",
    "happy_selected_words = [f[0] for f in happy_text.vocab().most_common(HAPPY_FREQUENCY_COUNT)]\n",
    "\n",
    "def happy_term_frequency(doc):\n",
    "    return [doc.count(word) for word in happy_selected_words]\n",
    "\n",
    "x_happy_train = [happy_term_frequency(d) for d,_ in train_happy_docs]\n",
    "x_happy_test = [happy_term_frequency(d) for d,_ in test_happy_docs]\n",
    "y_happy_train = [c for _,c in train_happy_docs]\n",
    "y_happy_test = [c for _,c in test_happy_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_happy_train = np.asarray(x_happy_train).astype('float32')\n",
    "x_happy_test = np.asarray(x_happy_test). astype('float32')\n",
    "\n",
    "y_happy_train = np.asarray(y_happy_train).astype('float32')\n",
    "y_happy_test = np.asarray(y_happy_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "happy_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(HAPPY_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "happy_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "\n",
    "happy_model.fit(x_happy_train, y_happy_train, epochs=50, batch_size=512)\n",
    "happy_results = happy_model.evaluate(x_happy_test, y_happy_test)\n",
    "\n",
    "happy_review = \"너무 화난다\"\n",
    "happy_token = happy_tokenize(happy_review)\n",
    "\n",
    "tf = happy_term_frequency(happy_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(happy_model.predict(data))\n",
    "\n",
    "#disgust emotion\n",
    "\n",
    "import pandas as pd\n",
    "train_disgust_df = pd.read_csv(\"disgusttrainalldata.txt\",\"\\t\")\n",
    "test_disgust_df = pd.read_csv(\"disgusttestalldata.txt\",\"\\t\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt_disgust = Okt()\n",
    "\n",
    "def disgust_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_disgust.pos(doc, norm=True, stem=False)]\n",
    "\n",
    "train_disgust_df.isnull().any()\n",
    "train_disgust_df['document'] = train_disgust_df['document'].fillna('');\n",
    "test_disgust_df.isnull().any()\n",
    "test_disgust_df['document'] = test_disgust_df['document'].fillna('');\n",
    "\n",
    "train_disgust_docs = [(disgust_tokenize(row[1]), row[2]) for row in train_disgust_df.values]\n",
    "test_disgust_docs = [(disgust_tokenize(row[1]),row[2]) for row in test_disgust_df.values]\n",
    "\n",
    "tokens = [t for d in train_disgust_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "disgust_text = nltk.Text(tokens, name='DISG')\n",
    "#print(len(disgust_text.tokens))\n",
    "#print(len(set(disgust_text.tokens)))\n",
    "#print(disgust_text.vocab().most_common(10))\n",
    "\n",
    "DISGUST_FREQUENCY_COUNT = 3000; #variable\n",
    "disgust_selected_words = [f[0] for f in disgust_text.vocab().most_common(DISGUST_FREQUENCY_COUNT)]\n",
    "\n",
    "def disgust_term_frequency(doc):\n",
    "    return [doc.count(word) for word in disgust_selected_words]\n",
    "\n",
    "x_disgust_train = [disgust_term_frequency(d) for d,_ in train_disgust_docs]\n",
    "x_disgust_test = [disgust_term_frequency(d) for d,_ in test_disgust_docs]\n",
    "y_disgust_train = [c for _,c in train_disgust_docs]\n",
    "y_disgust_test = [c for _,c in test_disgust_docs]\n",
    "\n",
    "import numpy as np\n",
    "x_disgust_train = np.asarray(x_disgust_train).astype('float32')\n",
    "x_disgust_test = np.asarray(x_disgust_test). astype('float32')\n",
    "\n",
    "y_disgust_train = np.asarray(y_disgust_train).astype('float32')\n",
    "y_disgust_test = np.asarray(y_disgust_test).astype('float32')\n",
    "\n",
    "import tensorflow as tf\n",
    "disgust_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(DISGUST_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "disgust_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "disgust_model.fit(x_disgust_train, y_disgust_train, epochs=50, batch_size=512)\n",
    "disgust_results = neutral_model.evaluate(x_disgust_test, y_disgust_test)\n",
    "disgust_review = \"너무 화난다\"\n",
    "disgust_token = disgust_tokenize(disgust_review)\n",
    "\n",
    "tf = disgust_term_frequency(disgust_token)\n",
    "data = np.expand_dims(np.asarray(tf).astype('float32'),axis=0)\n",
    "float(disgust_model.predict(data))\n",
    "\n",
    "#function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from math import pi\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "    \n",
    "def predict(predict):\n",
    "    fear_token = fear_tokenize(predict)\n",
    "    fear_tfq = fear_term_frequency(fear_token)\n",
    "    fear_data = np.expand_dims(np.asarray(fear_tfq).astype('float32'), axis=0)\n",
    "    fear_score = float(fear_model.predict(fear_data))\n",
    "    #print(f\"공포({round(fear_score*100)}%)\")\n",
    "    fear = round(fear_score*100)\n",
    "    surprise_token = surprise_tokenize(predict)\n",
    "    surprise_tfq = surprise_term_frequency(surprise_token)\n",
    "    surprise_data = np.expand_dims(np.asarray(surprise_tfq).astype('float32'), axis=0)\n",
    "    surprise_score = float(surprise_model.predict(surprise_data))\n",
    "    #print(f\"놀람({round(surprise_score*100)}%)\")\n",
    "    surprise = round(surprise_score*100)\n",
    "    anger_token = anger_tokenize(predict)\n",
    "    anger_tfq = anger_term_frequency(anger_token)\n",
    "    anger_data = np.expand_dims(np.asarray(anger_tfq).astype('float32'), axis=0)\n",
    "    anger_score = float(anger_model.predict(anger_data))\n",
    "    #print(f\"분노({round(anger_score*100)}%)\")\n",
    "    anger = round(anger_score*100)\n",
    "    sadness_token = sadness_tokenize(predict)\n",
    "    sadness_tfq = sadness_term_frequency(sadness_token)\n",
    "    sadness_data = np.expand_dims(np.asarray(sadness_tfq).astype('float32'), axis=0)\n",
    "    sadness_score = float(sadness_model.predict(sadness_data))\n",
    "    #print(f\"슬픔({round(sadness_score*100)}%)\")\n",
    "    sadness = round(sadness_score*100)\n",
    "    neutral_token = neutral_tokenize(predict)\n",
    "    neutral_tfq = neutral_term_frequency(neutral_token)\n",
    "    neutral_data = np.expand_dims(np.asarray(neutral_tfq).astype('float32'), axis=0)\n",
    "    neutral_score = float(neutral_model.predict(neutral_data))\n",
    "    #print(f\"중립({round(neutral_score*100)}%)\")\n",
    "    neutral = round(neutral_score*100)\n",
    "    happy_token = happy_tokenize(predict)\n",
    "    happy_tfq = happy_term_frequency(happy_token)\n",
    "    happy_data = np.expand_dims(np.asarray(happy_tfq).astype('float32'), axis=0)\n",
    "    happy_score = float(happy_model.predict(happy_data))\n",
    "    #print(f\"행복({round(happy_score*100)}%)\")\n",
    "    happy = round(happy_score*100)\n",
    "    disgust_token = disgust_tokenize(predict)\n",
    "    disgust_tfq = disgust_term_frequency(disgust_token)\n",
    "    disgust_data = np.expand_dims(np.asarray(disgust_tfq).astype('float32'), axis=0)\n",
    "    disgust_score = float(disgust_model.predict(disgust_data))\n",
    "    #print(f\"혐오({round(disgust_score*100)}%)\")\n",
    "    disgust = round(disgust_score*100)\n",
    "    total = fear+disgust+sadness+happy+anger+neutral+surprise\n",
    " \n",
    "    ## set data\n",
    "    df = pd.DataFrame({\n",
    "    'Character': ['RESULT'],\n",
    "    'Fear': [round(100/(total)*fear)],\n",
    "    'Disgust': [round(100/(total)*disgust)],\n",
    "    'Sadness': [round(100/(total)*sadness)],\n",
    "    'Happy': [round(100/(total)*happy)],\n",
    "    'Angry': [round(100/(total)*anger)],\n",
    "    'Neutral': [round(100/(total)*neutral)],\n",
    "    'Surprise': [round(100/(total)*surprise)],\n",
    "    })\n",
    "    labels = df.columns[1:]\n",
    "    num_labels = len(labels)\n",
    "    \n",
    "    angles = [x/float(num_labels)*(2*pi) for x in range(num_labels)] ## 각 등분점\n",
    "    angles += angles[:1] ## 시작점으로 다시 돌아와야하므로 시작점 추가\n",
    "    \n",
    "    my_palette = plt.cm.get_cmap(\"Set2\", len(df.index))\n",
    " \n",
    "    fig = plt.figure(figsize=(15,20))\n",
    "    fig.set_facecolor('white')\n",
    " \n",
    "    for i, row in df.iterrows():\n",
    "        color = my_palette(i)\n",
    "        data = df.iloc[i].drop('Character').tolist()\n",
    "        data += data[:1]\n",
    "    \n",
    "        ax = plt.subplot(3,2,i+1, polar=True)\n",
    "        ax.set_theta_offset(pi / 2) ## 시작점\n",
    "        ax.set_theta_direction(-1) ## 그려지는 방향 시계방향\n",
    "    \n",
    "        plt.xticks(angles[:-1], labels, fontsize=13) ## x축 눈금 라벨\n",
    "        ax.tick_params(axis='x', which='major', pad=15) ## x축과 눈금 사이에 여백을 준다.\n",
    " \n",
    "        ax.set_rlabel_position(0) ## y축 각도 설정(degree 단위)\n",
    "        plt.yticks([0,20,40,60,80,100],['0','20','40','60','80','100'], fontsize=10) ## y축 눈금 설정\n",
    "        plt.ylim(0,100)\n",
    "    \n",
    "        ax.plot(angles, data, color=color, linewidth=2, linestyle='solid') ## 레이더 차트 출력\n",
    "        ax.fill(angles, data, color=color, alpha=0.4) ## 도형 안쪽에 색을 채워준다.\n",
    "    \n",
    "        plt.title(row.Character, size=20, color=color,x=-0.2, y=1.2, ha='left') ## 타이틀은 캐릭터 클래스로 한다.\n",
    " \n",
    "        plt.tight_layout(pad=5) ## subplot간 패딩 조절\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"공포({round(100/(total)*fear)}%)\")\n",
    "    print(f\"혐오({round(100/(total)*disgust)}%)\")\n",
    "    print(f\"슬픔({round(100/(total)*sadness)}%)\")\n",
    "    print(f\"행복({round(100/(total)*happy)}%)\")\n",
    "    print(f\"분노({round(100/(total)*anger)}%)\")\n",
    "    print(f\"중립({round(100/(total)*neutral)}%)\")\n",
    "    print(f\"놀람({round(100/(total)*surprise)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-amazon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outside-tourism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-98d87b7e41d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"니가 왜 슬퍼\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "predict(\"니가 왜 슬퍼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "royal-palestine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "91/91 [==============================] - ETA: 0s - loss: 0.4815 - binary_accuracy: 0.774 - 1s 15ms/step - loss: 0.4777 - binary_accuracy: 0.7761\n",
      "Epoch 2/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.2721 - binary_accuracy: 0.8883\n",
      "Epoch 3/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.1644 - binary_accuracy: 0.9379\n",
      "Epoch 4/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0948 - binary_accuracy: 0.9668\n",
      "Epoch 5/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0581 - binary_accuracy: 0.9799\n",
      "Epoch 6/50\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.0379 - binary_accuracy: 0.9862\n",
      "Epoch 7/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0275 - binary_accuracy: 0.9895\n",
      "Epoch 8/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0220 - binary_accuracy: 0.9909\n",
      "Epoch 9/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0190 - binary_accuracy: 0.9920\n",
      "Epoch 10/50\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.0178 - binary_accuracy: 0.9920\n",
      "Epoch 11/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0166 - binary_accuracy: 0.9920\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0156 - binary_accuracy: 0.9931\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0152 - binary_accuracy: 0.9932\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0146 - binary_accuracy: 0.9930\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0143 - binary_accuracy: 0.9931\n",
      "Epoch 16/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0138 - binary_accuracy: 0.9935\n",
      "Epoch 17/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0138 - binary_accuracy: 0.9934\n",
      "Epoch 18/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0137 - binary_accuracy: 0.9936\n",
      "Epoch 19/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0130 - binary_accuracy: 0.9937\n",
      "Epoch 20/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0133 - binary_accuracy: 0.9937\n",
      "Epoch 21/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0131 - binary_accuracy: 0.9938\n",
      "Epoch 22/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0127 - binary_accuracy: 0.9939\n",
      "Epoch 23/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0128 - binary_accuracy: 0.9937\n",
      "Epoch 24/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0127 - binary_accuracy: 0.9939\n",
      "Epoch 25/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0126 - binary_accuracy: 0.9939\n",
      "Epoch 26/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0124 - binary_accuracy: 0.9940\n",
      "Epoch 27/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0124 - binary_accuracy: 0.9940\n",
      "Epoch 28/50\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.0125 - binary_accuracy: 0.9940\n",
      "Epoch 29/50\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.0124 - binary_accuracy: 0.9940\n",
      "Epoch 30/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0122 - binary_accuracy: 0.9940\n",
      "Epoch 31/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0121 - binary_accuracy: 0.9942\n",
      "Epoch 32/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0123 - binary_accuracy: 0.9941\n",
      "Epoch 33/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0121 - binary_accuracy: 0.9941\n",
      "Epoch 34/50\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.0121 - binary_accuracy: 0.9943\n",
      "Epoch 35/50\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.0121 - binary_accuracy: 0.9940\n",
      "Epoch 36/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0121 - binary_accuracy: 0.9940\n",
      "Epoch 37/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0120 - binary_accuracy: 0.9942\n",
      "Epoch 38/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0124 - binary_accuracy: 0.9941\n",
      "Epoch 39/50\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.0120 - binary_accuracy: 0.9942\n",
      "Epoch 40/50\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.0121 - binary_accuracy: 0.9942\n",
      "Epoch 41/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0120 - binary_accuracy: 0.9943\n",
      "Epoch 42/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0120 - binary_accuracy: 0.9941\n",
      "Epoch 43/50\n",
      "91/91 [==============================] - 1s 7ms/step - loss: 0.0119 - binary_accuracy: 0.9941\n",
      "Epoch 44/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0120 - binary_accuracy: 0.9943\n",
      "Epoch 45/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0122 - binary_accuracy: 0.9941\n",
      "Epoch 46/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0119 - binary_accuracy: 0.9943\n",
      "Epoch 47/50\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.0120 - binary_accuracy: 0.9943\n",
      "Epoch 48/50\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.0119 - binary_accuracy: 0.9944\n",
      "Epoch 49/50\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.0119 - binary_accuracy: 0.9940\n",
      "Epoch 50/50\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.0119 - binary_accuracy: 0.9944\n",
      "619/619 [==============================] - 2s 3ms/step - loss: 11.5650 - binary_accuracy: 0.6307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.932878851890564"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fear emotion\n",
    "\n",
    "#데이터 pandas로 읽기, 구문이 탭으로 구분되어있음.\n",
    "import pandas as pd\n",
    "train_fear_df = pd.read_csv(\"feartrainalldata6.txt\",\"\\t\") \n",
    "test_fear_df = pd.read_csv(\"feartestalldata6.txt\",\"\\t\")\n",
    "\n",
    "#konlpy는 띄어쓰기 알고리즘과 정규화를 이용해 맞춤법이 틀린 문장을 고치고 형태소 분석과 품사를 태깅해주는 클래스를 제공함.\n",
    "from konlpy.tag import Okt\n",
    "okt_fear = Okt()\n",
    "\n",
    "def fear_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_fear.pos(doc, norm=True, stem=False)]#08.22 stem=false로 바꿔봄. \n",
    "    #'구분자'.join(list) => 리스트 값과 값 사이에 구분자를 넣어 문자열을 하나로 합침. ex) '_'.join(['a','b','c'])이면 \"a_b_c\"로 반환\n",
    "    #pos(text) => 품사를 태깅한 상태로 명사를 변환, ex) 컴퓨터(x), 컵퓨터,noun(o)\n",
    "    #norm => 정규화 ex) 그래욬ㅋㅋㅋ => 그래요\n",
    "    #stem => 근어로 표현 ex) 그래요 => 그렇다\n",
    "\n",
    "train_fear_df.isnull().any()\n",
    "train_fear_df['document'] = train_fear_df['document'].fillna(''); #null값을 ''로 대체\n",
    "test_fear_df.isnull().any()\n",
    "test_fear_df['document'] = test_fear_df['document'].fillna('');\n",
    "\n",
    "train_fear_docs = [(fear_tokenize(row[1]), row[2]) for row in train_fear_df.values] #traindata 저장 row[1] : document / row[2] : label\n",
    "test_fear_docs = [(fear_tokenize(row[1]),row[2]) for row in test_fear_df.values] #testdata 저장\n",
    "\n",
    "tokens = [t for d in train_fear_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "fear_text = nltk.Text(tokens, name='FEAR') \n",
    "#문서 하나를 편리하게 탐색할 수 있는 기능 제공 (vocab().most_common() 사용)\n",
    "# print(len(fear_text.tokens)) #토큰 개수\n",
    "# print(len(set(fear_text.tokens))) #중복을 제외한 토큰 수\n",
    "# print(fear_text.vocab().most_common(10)) #출력빈도가 높은 상위 토큰 10개\n",
    "\n",
    "#countvectorization\n",
    "FEAR_FREQUENCY_COUNT = 3000; #자주 사용되는 토큰 설정\n",
    "fear_selected_words = [f[0] for f in fear_text.vocab().most_common(FEAR_FREQUENCY_COUNT)] #선택되어진 토큰들\n",
    "\n",
    "def fear_term_frequency(doc):\n",
    "    return [doc.count(word) for word in fear_selected_words]\n",
    "\n",
    "x_fear_train = [fear_term_frequency(d) for d,_ in train_fear_docs]\n",
    "x_fear_test = [fear_term_frequency(d) for d,_ in test_fear_docs]\n",
    "#x축에는 문서에 들어가는 단어 개수(단어들의 빈도수 정보)\n",
    "\n",
    "y_fear_train = [c for _,c in train_fear_docs]\n",
    "y_fear_test = [c for _,c in test_fear_docs]\n",
    "#y축에는 1 or 0, 분류 결과\n",
    "\n",
    "import numpy as np\n",
    "x_fear_train = np.asarray(x_fear_train).astype('float32')\n",
    "x_fear_test = np.asarray(x_fear_test). astype('float32')\n",
    "\n",
    "y_fear_train = np.asarray(y_fear_train).astype('float32')\n",
    "y_fear_test = np.asarray(y_fear_test).astype('float32')\n",
    "#np.asarray는 np.array와 달리 데이터 형태가 같을 때 복사하지 않음.\n",
    "#데이터 float로 형 변환\n",
    "\n",
    "import tensorflow as tf #텐서플로 케라스\n",
    "fear_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(FEAR_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "#레이어 구성은 dense층은 64개의 유닛, 활성함수는 relu, 마지막 층은 sigmoid 활성화 함수 사용\n",
    "\n",
    "fear_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "#손실 함수는 binary_crossentropy, RMSprop 옵티마이저를 통해 경사하강법 진행\n",
    "\n",
    "fear_model.fit(x_fear_train, y_fear_train, epochs=50, batch_size=512)\n",
    "#배치 사이즈 줄이면 한 번에 판단하는 데이터 수 증가함. 에포크 50번\n",
    "fear_results = fear_model.evaluate(x_fear_test, y_fear_test)\n",
    "\n",
    "fear_review = \"너무 무섭다\"\n",
    "fear_token = fear_tokenize(fear_review)\n",
    "\n",
    "fear_tf = fear_term_frequency(fear_token)\n",
    "f_data = np.expand_dims(np.asarray(fear_tf).astype('float32'),axis=0)\n",
    "float(fear_model.predict(f_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "secure-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "91/91 [==============================] - 2s 17ms/step - loss: 0.4629 - binary_accuracy: 0.7782\n",
      "Epoch 2/50\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.2446 - binary_accuracy: 0.9017\n",
      "Epoch 3/50\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.1382 - binary_accuracy: 0.9495\n",
      "Epoch 4/50\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.0791 - binary_accuracy: 0.9722\n",
      "Epoch 5/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0495 - binary_accuracy: 0.9823\n",
      "Epoch 6/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0344 - binary_accuracy: 0.9875\n",
      "Epoch 7/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0271 - binary_accuracy: 0.9896\n",
      "Epoch 8/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0217 - binary_accuracy: 0.9909\n",
      "Epoch 9/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0193 - binary_accuracy: 0.9917\n",
      "Epoch 10/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0174 - binary_accuracy: 0.9925\n",
      "Epoch 11/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0161 - binary_accuracy: 0.9930\n",
      "Epoch 12/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0158 - binary_accuracy: 0.9928\n",
      "Epoch 13/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0149 - binary_accuracy: 0.9932\n",
      "Epoch 14/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0142 - binary_accuracy: 0.9931\n",
      "Epoch 15/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0143 - binary_accuracy: 0.9934\n",
      "Epoch 16/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0138 - binary_accuracy: 0.9936\n",
      "Epoch 17/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0136 - binary_accuracy: 0.9935\n",
      "Epoch 18/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0135 - binary_accuracy: 0.9935\n",
      "Epoch 19/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0127 - binary_accuracy: 0.9939\n",
      "Epoch 20/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0128 - binary_accuracy: 0.9938\n",
      "Epoch 21/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0130 - binary_accuracy: 0.9938\n",
      "Epoch 22/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0129 - binary_accuracy: 0.9940\n",
      "Epoch 23/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0127 - binary_accuracy: 0.9938\n",
      "Epoch 24/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0124 - binary_accuracy: 0.9938\n",
      "Epoch 25/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0125 - binary_accuracy: 0.9942\n",
      "Epoch 26/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0126 - binary_accuracy: 0.9940\n",
      "Epoch 27/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0124 - binary_accuracy: 0.9941\n",
      "Epoch 28/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0124 - binary_accuracy: 0.9941\n",
      "Epoch 29/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0125 - binary_accuracy: 0.9939\n",
      "Epoch 30/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0121 - binary_accuracy: 0.9940\n",
      "Epoch 31/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0137 - binary_accuracy: 0.9937\n",
      "Epoch 32/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0129 - binary_accuracy: 0.9938\n",
      "Epoch 33/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0139 - binary_accuracy: 0.9941\n",
      "Epoch 34/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0118 - binary_accuracy: 0.9943\n",
      "Epoch 35/50\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.0141 - binary_accuracy: 0.9935\n",
      "Epoch 36/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0130 - binary_accuracy: 0.9939\n",
      "Epoch 37/50\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.0128 - binary_accuracy: 0.9938: 0s - loss: 0.0109 - binary_accuracy: 0.99 - ETA: 0s - loss: 0.0140 - binary_a\n",
      "Epoch 38/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0131 - binary_accuracy: 0.9941\n",
      "Epoch 39/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0134 - binary_accuracy: 0.9939\n",
      "Epoch 40/50\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.0127 - binary_accuracy: 0.9941\n",
      "Epoch 41/50\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.0128 - binary_accuracy: 0.9938\n",
      "Epoch 42/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0161 - binary_accuracy: 0.9937\n",
      "Epoch 43/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0117 - binary_accuracy: 0.9942\n",
      "Epoch 44/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0124 - binary_accuracy: 0.9940\n",
      "Epoch 45/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0146 - binary_accuracy: 0.9938\n",
      "Epoch 46/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0130 - binary_accuracy: 0.9940\n",
      "Epoch 47/50\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.0118 - binary_accuracy: 0.9943\n",
      "Epoch 48/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0165 - binary_accuracy: 0.9932\n",
      "Epoch 49/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0194 - binary_accuracy: 0.9930\n",
      "Epoch 50/50\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.0116 - binary_accuracy: 0.9942\n",
      "619/619 [==============================] - 2s 2ms/step - loss: 9.6555 - binary_accuracy: 0.6278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9988800883293152"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fear emotion\n",
    "\n",
    "#데이터 pandas로 읽기, 구문이 탭으로 구분되어있음.\n",
    "import pandas as pd\n",
    "train_fear_df = pd.read_csv(\"feartrainalldata6.txt\",\"\\t\") \n",
    "test_fear_df = pd.read_csv(\"feartestalldata6.txt\",\"\\t\")\n",
    "\n",
    "#konlpy는 띄어쓰기 알고리즘과 정규화를 이용해 맞춤법이 틀린 문장을 고치고 형태소 분석과 품사를 태깅해주는 클래스를 제공함.\n",
    "from konlpy.tag import Okt\n",
    "okt_fear = Okt()\n",
    "\n",
    "def fear_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_fear.pos(doc, norm=True, stem=False)]#08.22 stem=false로 바꿔봄. \n",
    "    #'구분자'.join(list) => 리스트 값과 값 사이에 구분자를 넣어 문자열을 하나로 합침. ex) '_'.join(['a','b','c'])이면 \"a_b_c\"로 반환\n",
    "    #pos(text) => 품사를 태깅한 상태로 명사를 변환, ex) 컴퓨터(x), 컵퓨터,noun(o)\n",
    "    #norm => 정규화 ex) 그래욬ㅋㅋㅋ => 그래요\n",
    "    #stem => 근어로 표현 ex) 그래요 => 그렇다\n",
    "\n",
    "train_fear_df.isnull().any()\n",
    "train_fear_df['document'] = train_fear_df['document'].fillna(''); #null값을 ''로 대체\n",
    "test_fear_df.isnull().any()\n",
    "test_fear_df['document'] = test_fear_df['document'].fillna('');\n",
    "\n",
    "train_fear_docs = [(fear_tokenize(row[1]), row[2]) for row in train_fear_df.values] #traindata 저장 row[1] : document / row[2] : label\n",
    "test_fear_docs = [(fear_tokenize(row[1]),row[2]) for row in test_fear_df.values] #testdata 저장\n",
    "\n",
    "tokens = [t for d in train_fear_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "fear_text = nltk.Text(tokens, name='FEAR') \n",
    "#문서 하나를 편리하게 탐색할 수 있는 기능 제공 (vocab().most_common() 사용)\n",
    "# print(len(fear_text.tokens)) #토큰 개수\n",
    "# print(len(set(fear_text.tokens))) #중복을 제외한 토큰 수\n",
    "# print(fear_text.vocab().most_common(10)) #출력빈도가 높은 상위 토큰 10개\n",
    "\n",
    "#countvectorization\n",
    "FEAR_FREQUENCY_COUNT = 3000; #자주 사용되는 토큰 설정\n",
    "fear_selected_words = [f[0] for f in fear_text.vocab().most_common(FEAR_FREQUENCY_COUNT)] #선택되어진 토큰들\n",
    "\n",
    "def fear_term_frequency(doc):\n",
    "    return [doc.count(word) for word in fear_selected_words]\n",
    "\n",
    "x_fear_train = [fear_term_frequency(d) for d,_ in train_fear_docs]\n",
    "x_fear_test = [fear_term_frequency(d) for d,_ in test_fear_docs]\n",
    "#x축에는 문서에 들어가는 단어 개수(단어들의 빈도수 정보)\n",
    "\n",
    "y_fear_train = [c for _,c in train_fear_docs]\n",
    "y_fear_test = [c for _,c in test_fear_docs]\n",
    "#y축에는 1 or 0, 분류 결과\n",
    "\n",
    "import numpy as np\n",
    "x_fear_train = np.asarray(x_fear_train).astype('float32')\n",
    "x_fear_test = np.asarray(x_fear_test). astype('float32')\n",
    "\n",
    "y_fear_train = np.asarray(y_fear_train).astype('float32')\n",
    "y_fear_test = np.asarray(y_fear_test).astype('float32')\n",
    "#np.asarray는 np.array와 달리 데이터 형태가 같을 때 복사하지 않음.\n",
    "#데이터 float로 형 변환\n",
    "\n",
    "import tensorflow as tf #텐서플로 케라스\n",
    "fear_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(FEAR_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "#레이어 구성은 dense층은 64개의 유닛, 활성함수는 relu, 마지막 층은 sigmoid 활성화 함수 사용\n",
    "\n",
    "fear_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "#손실 함수는 binary_crossentropy, RMSprop 옵티마이저를 통해 경사하강법 진행\n",
    "\n",
    "fear_model.fit(x_fear_train, y_fear_train, epochs=50, batch_size=512)\n",
    "#배치 사이즈 줄이면 한 번에 판단하는 데이터 수 증가함. 에포크 50번\n",
    "fear_results = fear_model.evaluate(x_fear_test, y_fear_test)\n",
    "\n",
    "fear_review = \"너무 무섭다\"\n",
    "fear_token = fear_tokenize(fear_review)\n",
    "\n",
    "fear_tf = fear_term_frequency(fear_token)\n",
    "f_data = np.expand_dims(np.asarray(fear_tf).astype('float32'),axis=0)\n",
    "float(fear_model.predict(f_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fear emotion\n",
    "\n",
    "#데이터 pandas로 읽기, 구문이 탭으로 구분되어있음.\n",
    "import pandas as pd\n",
    "train_fear_df = pd.read_csv(\"feartrainalldata6.txt\",\"\\t\") \n",
    "test_fear_df = pd.read_csv(\"feartestalldata6.txt\",\"\\t\")\n",
    "\n",
    "#konlpy는 띄어쓰기 알고리즘과 정규화를 이용해 맞춤법이 틀린 문장을 고치고 형태소 분석과 품사를 태깅해주는 클래스를 제공함.\n",
    "from konlpy.tag import Okt\n",
    "okt_fear = Okt()\n",
    "\n",
    "def fear_tokenize(doc):\n",
    "    return ['/'.join(t) for t in okt_fear.pos(doc, norm=True, stem=False)]#08.22 stem=false로 바꿔봄. \n",
    "    #'구분자'.join(list) => 리스트 값과 값 사이에 구분자를 넣어 문자열을 하나로 합침. ex) '_'.join(['a','b','c'])이면 \"a_b_c\"로 반환\n",
    "    #pos(text) => 품사를 태깅한 상태로 명사를 변환, ex) 컴퓨터(x), 컵퓨터,noun(o)\n",
    "    #norm => 정규화 ex) 그래욬ㅋㅋㅋ => 그래요\n",
    "    #stem => 근어로 표현 ex) 그래요 => 그렇다\n",
    "\n",
    "train_fear_df.isnull().any()\n",
    "train_fear_df['document'] = train_fear_df['document'].fillna(''); #null값을 ''로 대체\n",
    "test_fear_df.isnull().any()\n",
    "test_fear_df['document'] = test_fear_df['document'].fillna('');\n",
    "\n",
    "train_fear_docs = [(fear_tokenize(row[1]), row[2]) for row in train_fear_df.values] #traindata 저장 row[1] : document / row[2] : label\n",
    "test_fear_docs = [(fear_tokenize(row[1]),row[2]) for row in test_fear_df.values] #testdata 저장\n",
    "\n",
    "tokens = [t for d in train_fear_docs for t in d[0]]\n",
    "\n",
    "import nltk\n",
    "fear_text = nltk.Text(tokens, name='FEAR') \n",
    "#문서 하나를 편리하게 탐색할 수 있는 기능 제공 (vocab().most_common() 사용)\n",
    "# print(len(fear_text.tokens)) #토큰 개수\n",
    "# print(len(set(fear_text.tokens))) #중복을 제외한 토큰 수\n",
    "# print(fear_text.vocab().most_common(10)) #출력빈도가 높은 상위 토큰 10개\n",
    "\n",
    "#countvectorization\n",
    "FEAR_FREQUENCY_COUNT = 3000; #자주 사용되는 토큰 설정\n",
    "fear_selected_words = [f[0] for f in fear_text.vocab().most_common(FEAR_FREQUENCY_COUNT)] #선택되어진 토큰들\n",
    "\n",
    "def fear_term_frequency(doc):\n",
    "    return [doc.count(word) for word in fear_selected_words]\n",
    "\n",
    "x_fear_train = [fear_term_frequency(d) for d,_ in train_fear_docs]\n",
    "x_fear_test = [fear_term_frequency(d) for d,_ in test_fear_docs]\n",
    "#x축에는 문서에 들어가는 단어 개수(단어들의 빈도수 정보)\n",
    "\n",
    "y_fear_train = [c for _,c in train_fear_docs]\n",
    "y_fear_test = [c for _,c in test_fear_docs]\n",
    "#y축에는 1 or 0, 분류 결과\n",
    "\n",
    "import numpy as np\n",
    "x_fear_train = np.asarray(x_fear_train).astype('float32')\n",
    "x_fear_test = np.asarray(x_fear_test). astype('float32')\n",
    "\n",
    "y_fear_train = np.asarray(y_fear_train).astype('float32')\n",
    "y_fear_test = np.asarray(y_fear_test).astype('float32')\n",
    "#np.asarray는 np.array와 달리 데이터 형태가 같을 때 복사하지 않음.\n",
    "#데이터 float로 형 변환\n",
    "\n",
    "import tensorflow as tf #텐서플로 케라스\n",
    "fear_model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation = 'relu',input_shape=(FEAR_FREQUENCY_COUNT,)),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "#레이어 구성은 dense층은 64개의 유닛, 활성함수는 relu, 마지막 층은 sigmoid 활성화 함수 사용\n",
    "\n",
    "fear_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "             loss=tf.keras.losses.binary_crossentropy,\n",
    "             metrics=[tf.keras.metrics.binary_accuracy]\n",
    "             )\n",
    "#손실 함수는 binary_crossentropy, RMSprop 옵티마이저를 통해 경사하강법 진행\n",
    "\n",
    "fear_model.fit(x_fear_train, y_fear_train, epochs=50, batch_size=512)\n",
    "#배치 사이즈 줄이면 한 번에 판단하는 데이터 수 증가함. 에포크 50번\n",
    "fear_results = fear_model.evaluate(x_fear_test, y_fear_test)\n",
    "\n",
    "fear_review = \"너무 무섭다\"\n",
    "fear_token = fear_tokenize(fear_review)\n",
    "\n",
    "fear_tf = fear_term_frequency(fear_token)\n",
    "f_data = np.expand_dims(np.asarray(fear_tf).astype('float32'),axis=0)\n",
    "float(fear_model.predict(f_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
